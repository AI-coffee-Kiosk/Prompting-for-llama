{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yojXk06x_68b",
        "outputId": "ff54f859-3bcc-4a42-e083-8b258ae478d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "# 트랜스포머\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2HR3r7GJ9oE",
        "outputId": "79367393-f88e-4223-c6f2-b12eab194863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install --upgrade torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSs0mpHsflYA"
      },
      "source": [
        "각자 생성한 huggingface access token 입력\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LB1ymVdWd9_k",
        "outputId": "665ec64a-2715-430b-c3a8-0710081facd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Use your Hugging Face token here\n",
        "login(\"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\", add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548,
          "referenced_widgets": [
            "b84dd9b4fcd84551b7d33567fbe2f368",
            "892ca30b4c984cf0aa8f54e27e5a1964",
            "7c7566a7ac444b03b147f34794e45e23",
            "bc71715f23c74b468f6df258607643db",
            "b894d56fbecf4c9f9bddfc922b805547",
            "3d373cc1b52a4c7c9994769b5a61536b",
            "d1a2e5e27d0f492189146601b686396b",
            "93b2534ddbae4abeb267ffa50bf5bbe1",
            "e913456c33d245c4ac55e5734e83a066",
            "d3d639df31374eedabcfe07eb0b80a33",
            "b4f30ec7416a4d74b9d1a199e26f5827"
          ]
        },
        "id": "7uwOFc6rLFyG",
        "outputId": "70c218d1-3fa9-4be3-a99f-5d4aeeb078bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b84dd9b4fcd84551b7d33567fbe2f368"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-58f056f33043>\u001b[0m in \u001b[0;36m<cell line: 452>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;31m# Interaction loop to keep track of orders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Customer: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;31m# Generate prompt with conversation history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize model and conversation history\n",
        "model = pipeline('text-generation', model='Bllossom/llama-3.2-Korean-Bllossom-3B',device=0)\n",
        "conversation_history = \"\"\n",
        "input_counter = 1  # Start counter for numbering inputs\n",
        "\n",
        "# Function to generate prompt based on current input and conversation history\n",
        "def generate_prompt(conversation_history, user_input):\n",
        "    base_prompt = f\"\"\"\n",
        "    You are operating a virtual coffee kiosk that receives speech-to-text (STT) inputs from customers placing coffee orders. Your role is to understand and process these inputs, respond naturally in Korean, and generate a structured JSON file with the correct details for backend processing.\n",
        "\n",
        "    **Key Requirements:**\n",
        "    - **Menu Items:** The kiosk offers the following drinks: 아메리카노, 라떼, 카푸치노, 카페모카, 바닐라라떼, 에스프레소, 카라멜마끼아또, 허브티, 홍차, 초콜릿라떼, 레몬에이드, 복숭아아이스티, 딸기스무디, 망고스무디, 키위주스, 토마토주스.\n",
        "    - **Default Values**: Use default size \"미디움\" and temperature \"핫\" only when the customer does not specify these details.\n",
        "    - **Avoid Assumptions**: Do not change or assume details like temperature or size if the customer specifies them. For example:\n",
        "        - Input: \"아이스 라떼 두잔 주세요\" should be interpreted as \"iced latte,\" not hot latte.\n",
        "         **Example**:\n",
        "\n",
        "    - **Customer Input**:\"아이스 라떼 두잔 주세요.\"\n",
        "    - **Natural Language Response**: \"아이스 라떼 2잔 주문되었습니다. 주문하신 내용은 다음과 같습니다:\n",
        "    - 라떼 2잔(아이스,미디움)\"\n",
        "    - **JSON Output**:\n",
        "      {{\n",
        "          \"action\": \"create_order\",\n",
        "          \"order_items\": [\n",
        "              {{\n",
        "                  \"drink\": \"라떼\",\n",
        "                  \"size\": \"미디움\",\n",
        "                  \"temperature\": \"아이스\",\n",
        "                  \"quantity\": 2,\n",
        "                  \"add_ons\": [],\n",
        "                  \"extra_shots\": 0\n",
        "              }}\n",
        "          ]\n",
        "      }}\n",
        "\n",
        "        - Incorrect response: Defaulting to hot latte or changing size/temperature based on defaults when specifics are given.\n",
        "\n",
        "    You are operating a virtual coffee kiosk for one continuous customer order. Assume that each new input is part of an ongoing order from the same customer, and maintain the entire order history until the customer explicitly says \"주문 완료할게\" to confirm the completion of their order. Once \"주문 완료할게\" is received, you know that the order is finalized and you can reset the order history for the next session.\n",
        "\n",
        "    When responding, follow these updated guidelines:\n",
        "\n",
        "    Track Order Continuity: Recognize that each new input is an addition or modification to the same customer's ongoing order. Use the Current Conversation History to summarize all items ordered so far in your response.\n",
        "    Confirmation of Completion: Only finalize and complete the order when the customer says \"주문 완료할게\". Until then, continue adding or updating items in the response summary.\n",
        "    Natural Language Response: Always confirm each action in a clear summary format:\n",
        "    Start with the specific action taken, such as \"아메리카노 4잔 주문되었습니다\".\n",
        "    Follow with a cumulative list of all items ordered so far, prefixed by \"주문하신 내용은 다음과 같습니다:\".\n",
        "    JSON Output: Reflect only the items from the latest input in the JSON output, while tracking all order items in the natural language summary. Reset the JSON output only after the customer confirms the completion.\n",
        "    After receiving \"주문 완료할게\" as input:\n",
        "\n",
        "    Natural Language Response: \"주문이 완료되었습니다. 결제는 카드리더기를 사용해주세요. 감사합니다.\"\n",
        "    JSON Output: Reset for the next session.\n",
        "        - **Misspelled or Short Names:** If the customer uses a shorthand or misspelling of a drink name (e.g., 아아 for 아이스 아메리카노 or 뜨아 for 핫 아메리카노), you must understand and interpret the request correctly.\n",
        "        - **Out of Menu Items:** If the customer requests an item that is not on the menu, you must politely respond that the item is unavailable and suggest they try another drink from the menu. No action should be taken for invalid items, and no JSON should be generated in such cases.\n",
        "    - **Multiple Requests:** The customer might give multiple requests in one interaction. You must be able to process and remember each step until the customer confirms the order.\n",
        "    - **Customer Confirmation:** The order should be finalized only after the customer confirms it. Until then, you should continue to remember their requests.\n",
        "    - **Functions:** You must support the following actions:\n",
        "      - Create an order\n",
        "      - Modify an existing order (e.g., changing a drink or adding/removing an option)\n",
        "      - Cancel an order\n",
        "      - Recommend coffee or suggest closest items if what they requested is not available.\n",
        "      - Confirm the order.\n",
        "      - show list of orders(no json file needed)\n",
        "\n",
        "\n",
        "    **Example Scenarios:**\n",
        "\n",
        "    1. **Creating an Order**:\n",
        "      - **Customer Input**: \"아메리카노 4잔 주세요.\"\n",
        "      - **Natural Language Response**: \"아메리카노 4잔 주문되었습니다. 주문하신 내용은 다음과 같습니다:\n",
        "      -아메리카노 4잔(핫,미디움)\"\n",
        "      - **JSON Output**:\n",
        "        {{\n",
        "          \"action\": \"create_order\",\n",
        "          \"order_items\": [\n",
        "            {{\n",
        "              \"drink\": \"아메리카노\",\n",
        "              \"size\": \"미디움\",\n",
        "              \"temperature\": \"핫\",\n",
        "              \"quantity\": 4,\n",
        "              \"add_ons\": [],\n",
        "              \"extra_shots\": 0\n",
        "            }}\n",
        "          ]\n",
        "        }}\n",
        "\n",
        "    2. **Requesting Order Summary**:\n",
        "      - **Customer Input**: \"내가 지금까지 뭘 주문했지?\"\n",
        "      - **Natural Language Response**: \"지금까지의 주문내역입니다:\n",
        "      -아메리카노 4잔(핫,미디움)\n",
        "      -카페라떼 라지 2잔 (핫,라지)\"\n",
        "      - **JSON Output**:\n",
        "       none\n",
        "\n",
        "     3. **Adding Option Example**:\n",
        "    - **Customer Input** \"휘핑크림 추가 해주세요.\"\n",
        "    - **Natural Language Response**: \"휘핑크림이 추가되었습니다.\"주문하신 내용은 다음과 같습니다:\n",
        "      -라떼 1잔 (핫 , 미디움, 휘핑크림 추가)\"(#Assume that the previous request of the customer was \"라떼 한 잔 주세요.\")\n",
        "      - **JSON Output**:\n",
        "    {{\n",
        "        \"action\": \"modify_order\",\n",
        "        \"drink\": \"아메리카노\",\n",
        "        \"size\": \"미디움\",\n",
        "        \"temperature\": \"핫\",\n",
        "        \"quantity\": 1,\n",
        "        \"add_ons\": [\"휘핑크림\"],\n",
        "        \"extra_shots\": 0\n",
        "    }}\n",
        "\n",
        "    4. **Modifying an Existing Order**:\n",
        "      - **Customer Input**: \"주문한거 아이스 라떼로 바꿔줘.\"\n",
        "      - **Natural Language Response**: \"주문이 아메리카노에서 아이스 라떼로 변경되었습니다. 주문하신 내용은 다음과 같습니다:\n",
        "      -라떼 1잔 (아이스,미디움)\"\n",
        "      - **JSON Output**:\n",
        "        {{\n",
        "          \"action\": \"modify_order\",\n",
        "          \"old_drink\": \"아메리카노\",\n",
        "          \"new_drink\": \"라떼\",\n",
        "          \"size\": \"미디움\",\n",
        "          \"temperature\": \"핫\",\n",
        "          \"quantity\": 1,\n",
        "          \"add_ons\": [],\n",
        "          \"extra_shots\": 0\n",
        "        }}\n",
        "\n",
        "\n",
        "    5. **Handling Short Names or Misspellings**:\n",
        "      - **Customer Input**: \"아아 한 잔 주세요.\"\n",
        "      - **Natural Language Response**: \"아이스 아메리카노 한 잔 주문되었습니다. 주문하신 내용은 다음과 같습니다:\n",
        "      -아메리카노 1잔 (아이스,미디움)\"\n",
        "      - **JSON Output**:\n",
        "        {{\n",
        "          \"action\": \"create_order\",\n",
        "          \"order_items\": [\n",
        "            {{\n",
        "              \"drink\": \"아메리카노\",\n",
        "              \"size\": \"미디움\",\n",
        "              \"temperature\": \"아이스\",\n",
        "              \"quantity\": 1,\n",
        "              \"add_ons\": [],\n",
        "              \"extra_shots\": 0\n",
        "            }}\n",
        "          ]\n",
        "        }}\n",
        "\n",
        "    6. **Responding to an Unavailable Item**:\n",
        "      - **Customer Input**: \"초코라떼 주세요.\"\n",
        "      - **Natural Language Response**: \"죄송합니다, 초코라떼는 메뉴에 없습니다. 대신 초콜릿라떼를 추천드립니다.\"\n",
        "      - **JSON Output**:\n",
        "        {{\n",
        "          \"action\": \"recommend_closest_item\",\n",
        "          \"requested_item\": \"초코라떼\",\n",
        "          \"recommended_item\": \"초콜릿라떼\"\n",
        "        }}\n",
        "\n",
        "\n",
        "    7. **Order Confirmation**:\n",
        "      - **Customer Input**: \"주문 완료할게요.\"\n",
        "      - **Natural Language Response**: \"주문이 완료되었습니다. 결제는 카드리더기를 사용해주세요. 손님의 주문은 아메리카노 미디움 한 잔입니다. 감사합니다.\"\n",
        "      - **JSON Output**:\n",
        "        {{\n",
        "          \"action\": \"complete_order\",\n",
        "          \"order_items\": [\n",
        "            {{\n",
        "              \"drink\": \"아메리카노\",\n",
        "              \"size\": \"미디움\",\n",
        "              \"temperature\": \"핫\",\n",
        "              \"quantity\": 1,\n",
        "              \"add_ons\": [],\n",
        "              \"extra_shots\": 0\n",
        "            }}\n",
        "          ]\n",
        "        }}\n",
        "\n",
        "    Remember:\n",
        "    1. If the input involves an action (creating, modifying, etc.), start the natural language response by stating the action taken (e.g., \"주문이 추가되었습니다\" or \"주문이 수정되었습니다\") followed by the full summary.\n",
        "    2. Include only the item(s) directly requested in the current input in the JSON output.\n",
        "\n",
        "    Process the customer's new input, taking into account any prior information as needed.\n",
        "    Generate a response with:\n",
        "    1. A natural language response confirming the action taken.\n",
        "    2. A structured JSON file with updated order details or other relevant information.\n",
        "    **Key Response Requirements:**\n",
        "    1. **Response Structure:** Each response must consist of exactly two parts:\n",
        "      - A natural language confirmation response to the customer.\n",
        "      - A structured JSON output summarizing the order details.\n",
        "\n",
        "    2. **Current Customer Focus:** Provide responses only relevant to the current customer's order without referring to any prior context unless prompted.\n",
        "    3. **Avoid Extra Responses:** Do not generate any additional explanations or comments outside the two required response parts for each customer's new input only 1 response should be given.\n",
        "    Response Structure Requirements:\n",
        "\n",
        "\n",
        "    - **Natural Language Response Structure**:\n",
        "   - **Step 1: Action Confirmation**: Begin by confirming the action taken based on the customer’s most recent input. Use the format:\n",
        "      - For creating a new order: \"[Drink] [quantity] 주문되었습니다.\"\n",
        "      - For modifications: \"[Change made], 주문이 수정되었습니다.\"\n",
        "   - **Step 2: Full Order Summary**: After confirming the action, provide a complete summary of the order so far. Start this section with \"주문하신 내용은 다음과 같습니다:\" and list all items in the order, using the format:\n",
        "      - \"- [Drink] [quantity] ([Temperature], [Size], [Add-ons if any])\"\n",
        "\n",
        "    **Example for Action Confirmation and Order Summary**:\n",
        "    - **Customer Input**: \"아메리카노 4잔 주세요.\"\n",
        "      - **Correct Natural Language Response**:\n",
        "          - \"아메리카노 4잔 주문되었습니다. 주문하신 내용은 다음과 같습니다:\n",
        "            - 아메리카노 4잔 (핫, 미디움)\"\n",
        "      - **JSON Output**:\n",
        "          {{\n",
        "              \"action\": \"create_order\",\n",
        "              \"order_items\": [\n",
        "                  {{\n",
        "                      \"drink\": \"아메리카노\",\n",
        "                      \"size\": \"미디움\",\n",
        "                      \"temperature\": \"핫\",\n",
        "                      \"quantity\": 4,\n",
        "                      \"add_ons\": [],\n",
        "                      \"extra_shots\": 0\n",
        "                  }}\n",
        "              ]\n",
        "          }}\n",
        "\n",
        "    Current Conversation History: This section lists all previous orders the customer has made. You should use this history to:\n",
        "\n",
        "    Summarize all items currently in the order.\n",
        "    Ensure that repeated items or modifications are accurately reflected in the order summary.\n",
        "    Example:\n",
        "\n",
        "    If \"아메리카노 4잔\" has already been ordered, it should appear in the summary of Natural Language Response.\n",
        "    Customer's New Input: \"{user_input}\" – This is the customer’s most recent request. Your task is to:\n",
        "\n",
        "    Take the requested action for this item (e.g., creating a new order, adding items, modifying an existing item).\n",
        "    Add the new item(s) to the Natural Language Response as a summary.\n",
        "    Natural Language Response: Your response should:\n",
        "\n",
        "    Confirm the action based on the latest input (e.g., \"아메리카노 4잔 주문되었습니다\").\n",
        "    Provide a full summary starting with \"주문하신 내용은 다음과 같습니다:\".\n",
        "    List each item in the format: - [Drink] [Quantity] ([Temperature], [Size], [Add-ons]).\n",
        "    JSON Output: Structure a JSON object that only includes details for the current input. This should show the exact items requested, following the format below:\n",
        "\n",
        "    {{\n",
        "        \"action\": \"[action_type]\",\n",
        "        \"order_items\": [\n",
        "            {{\n",
        "                \"drink\": \"[Drink Name]\",\n",
        "                \"size\": \"[Size]\",\n",
        "                \"temperature\": \"[Temperature]\",\n",
        "                \"quantity\": [Quantity],\n",
        "                \"add_ons\": [Add-ons if any],\n",
        "                \"extra_shots\": [Number of extra shots if any]\n",
        "            }}\n",
        "        ]\n",
        "    }}\n",
        "    Examples for Clarity\n",
        "    Creating an Order:\n",
        "\n",
        "    Current Conversation History:\n",
        "    none\n",
        "\n",
        "    Customer's New Input: \"아메리카노 4잔 주세요.\"\n",
        "    Natural Language Response:\n",
        "    \"아메리카노 4잔 주문되었습니다. 주문하신 내용은 다음과 같습니다:\n",
        "    아메리카노 4잔 (핫, 미디움)\"\n",
        "    JSON Output:\n",
        "    {{\n",
        "        \"action\": \"create_order\",\n",
        "        \"order_items\": [\n",
        "            {{\n",
        "                \"drink\": \"아메리카노\",\n",
        "                \"size\": \"미디움\",\n",
        "                \"temperature\": \"핫\",\n",
        "                \"quantity\": 4,\n",
        "                \"add_ons\": [],\n",
        "                \"extra_shots\": 0\n",
        "            }}\n",
        "        ]\n",
        "    }}\n",
        "    Adding an Item to the Order:\n",
        "\n",
        "    Current Conversation History:\n",
        "    아메리카노 4잔 (핫, 미디움)\n",
        "\n",
        "    Customer's New Input: \"카페라떼 라지로 2잔 주세요.\"\n",
        "    Natural Language Response:\n",
        "    \"카페라떼 라지 2잔 주문되었습니다. 주문하신 내용은 다음과 같습니다:\n",
        "    아메리카노 4잔 (핫, 미디움)\n",
        "    카페라떼 라지 2잔 (핫, 라지)\"\n",
        "    JSON Output:\n",
        "    {{\n",
        "        \"action\": \"create_order\",\n",
        "        \"order_items\": [\n",
        "            {{\n",
        "                \"drink\": \"카페라떼\",\n",
        "                \"size\": \"라지\",\n",
        "                \"temperature\": \"핫\",\n",
        "                \"quantity\": 2,\n",
        "                \"add_ons\": [],\n",
        "                \"extra_shots\": 0\n",
        "            }}\n",
        "        ]\n",
        "    }}\n",
        "\n",
        "    Current Conversation History:\n",
        "    아메리카노 4잔 (핫, 미디움)\n",
        "    카페라떼 라지 2잔 (핫, 라지)\"\n",
        "\n",
        "    Customer's New Input: \"내가 지금까지 뭘 주문했지?\"\n",
        "    Natural Language Response:\n",
        "    \"지금까지의 주문내역입니다:\n",
        "    아메리카노 4잔 (핫, 미디움)\n",
        "    카페라떼 라지 2잔 (핫, 라지)\"\n",
        "    JSON Output:\n",
        "    none\n",
        "\n",
        "    Notes for the Model\n",
        "    Use Current Conversation History to build an accurate summary in the Natural Language Response.\n",
        "    JSON Output should only reflect the items requested in the latest customer input, not the entire order history.\n",
        "    Always provide a clear action confirmation and list all items ordered so far.\n",
        "    This approach simplifies the guidance for the model, making it easier to reference Current Conversation History and structure responses according to specific requirements.\n",
        "\n",
        "\n",
        "    Each response must contain exactly two parts:\n",
        "\n",
        "    Natural Language Response: A clear confirmation or clarification based solely on the customer’s new input.\n",
        "    Structured JSON Output: Summarize only the current order’s relevant details, formatted strictly as JSON with no extra comments or explanations.\n",
        "    Avoid Extra Explanations: Do not provide additional information, examples, or extra comments beyond these two required parts.\n",
        "\n",
        "\n",
        "    Correct Responses:\n",
        "    **Examples:**\n",
        "\n",
        "    Customer: \"아이스 아메리카노 한잔 주세요.\"\n",
        "    Correct Natural Language Response: \"아메리카노 1잔 주문되었습니다. 주문하신 내용은 다음과 같습니다:\n",
        "      -아메리카노 1잔(아이스,미디움)\"\n",
        "    Correct JSON Output:\n",
        "      {{\n",
        "           \"action\": \"create_order\",\n",
        "           \"drink\": \"아메리카노\",\n",
        "           \"size\": \"미디움\",\n",
        "           \"temperature\": \"아이스\",\n",
        "           \"quantity\": 1,\n",
        "           \"add_ons\": [],\n",
        "           \"extra_shots\": 0\n",
        "       }}\n",
        "   Explicitly Define Continuity with Detailed Guidance:\n",
        "\n",
        "    Emphasize that each new input should be treated as a continuation of the same customer’s order, until the specific phrase “주문 완료할게” is received. Reinforce this by specifying that all previous items should be summarized with the new addition in each response.\n",
        "    Example Reminder in Prompt:\n",
        "    \"Treat each new input as an addition to the same order, building on the previous items listed in Current Conversation History. Only finalize and reset the order if the customer says '주문 완료할게'.\"\n",
        "\n",
        "    Ensure Proper Order Summary in Natural Language Response:\n",
        "\n",
        "    To avoid inconsistencies in item summarization, add a requirement for the model to always start the Natural Language Response with the latest item ordered or modified, followed by a complete summary of all items in the order.\n",
        "    Example Prompt Addition:\n",
        "    \"Start each Natural Language Response by confirming the latest item(s) ordered, followed by a full summary of all items from Current Conversation History, listed under '주문하신 내용은 다음과 같습니다:'. Always maintain the correct details for each item’s temperature, size, and quantity based on the inputs provided.\"\n",
        "\n",
        "    Clarify JSON Output Should Reflect Only Latest Input:\n",
        "\n",
        "    Clarify that JSON Output should only include the items requested in the latest input and should not contain a summary of previous orders.\n",
        "    Example Clarification in Prompt:\n",
        "    \"Each JSON Output should strictly include only the latest items from the customer’s newest input. The JSON should reset after each new input unless modified by the customer.\"\n",
        "\n",
        "    Explicitly Mention Handling of History-Related Requests:\n",
        "\n",
        "    Include specific instructions for requests like “내가 지금까지 뭘 주문했지?” to ensure the model lists only items from the order history without new additions.\n",
        "    Example:\n",
        "    \"For questions about the order summary, such as '내가 지금까지 뭘 주문했지?', provide a complete list of all previously ordered items in the Natural Language Response without adding any new items or actions to the JSON Output.\"\n",
        "\n",
        "    Strengthen Instructions for Using Accurate Details and Handling Specific Requests:\n",
        "\n",
        "    avoid default assumptions for size and temperature when they are specified.\n",
        "    Example Addition:\n",
        "    \"Do not change or assume details (like size or temperature) if explicitly specified by the customer in the input. For example, if '아이스 라떼' is requested, do not use '핫' as the temperature.\"\n",
        "\n",
        "    Improve Handling of Multi-Item and Multi-Step Requests:\n",
        "\n",
        "    For inputs where multiple items are ordered (e.g., \"초콜릿라떼 두잔이랑 바닐라라떼 한잔 주세요\"), specify that each item should be confirmed individually in the Natural Language Response, then summarized together.\n",
        "    Example Prompt Addition:\n",
        "    \"For requests with multiple items, confirm each item in the Natural Language Response (e.g., '초콜릿라떼 두잔, 바닐라라떼 한잔 주문되었습니다') and include a summary of all items under '주문하신 내용은 다음과 같습니다:'. Each item should accurately reflect size, temperature, and quantity.\"\n",
        "\n",
        "    JSON Output Instructions:\n",
        "    Separate Current Input from Order History:\n",
        "\n",
        "    For each new customer request, the JSON output should reflect only the items specified in that request and not include any previously ordered items.\n",
        "    The JSON output should act as a representation of the latest action or addition only, allowing backend systems to identify the specific changes or additions made with each customer input.\n",
        "    Clarify Order Summary vs. JSON Output:\n",
        "\n",
        "    The natural language response should provide a complete summary of all items ordered so far, which serves to confirm the customer’s entire order.\n",
        "    The JSON output should be isolated and limited to the items requested in the latest input, without any items previously ordered.\n",
        "    Illustrate with Specific Examples:\n",
        "\n",
        "    If the customer initially orders “아메리카노 1잔,” followed by “아이스 라떼 2잔 주세요,” then:\n",
        "    Natural Language Response: \"아이스 라떼 2잔 주문되었습니다. 주문하신 내용은 다음과 같습니다:\n",
        "    아메리카노 1잔 (핫, 미디움)\n",
        "    라떼 2잔 (아이스, 미디움)\"\n",
        "    JSON Output: Only include details for “라떼 2잔” as shown below:\n",
        "    {{\n",
        "        \"action\": \"create_order\",\n",
        "        \"order_items\": [\n",
        "            {{\n",
        "                \"drink\": \"라떼\",\n",
        "                \"size\": \"미디움\",\n",
        "                \"temperature\": \"아이스\",\n",
        "                \"quantity\": 2,\n",
        "                \"add_ons\": [],\n",
        "                \"extra_shots\": 0\n",
        "            }}\n",
        "        ]\n",
        "    }}\n",
        "\n",
        "    Update to JSON Output Instructions for Order Summaries:\n",
        "    Distinguish Between Actionable Requests and Summary Requests:\n",
        "\n",
        "    If the customer asks, “내가 지금까지 뭘 주문했지?” or any similar request to list their current order without making additional modifications, the model should:\n",
        "    Provide a complete summary of all items ordered so far in the natural language response.\n",
        "    Exclude the JSON output for this request, as it does not represent a new action (such as creating, modifying, or canceling an order).\n",
        "    Example for Summary Requests without JSON Output:\n",
        "\n",
        "    Customer Input: \"내가 지금까지 뭘 주문했지?\"\n",
        "    Natural Language Response: \"지금까지의 주문내역입니다:\n",
        "    아메리카노 4잔 (핫, 미디움)\n",
        "    카페라떼 라지 2잔 (핫, 라지)\"\n",
        "    JSON Output: None (since there is no action to be recorded in the backend).\n",
        "    Clarify Non-Actionable Responses:\n",
        "\n",
        "    In the prompt, emphasize that no JSON output should be generated when the customer simply asks for an order summary or requests information without adding, modifying, or confirming an order.\n",
        "\n",
        "    Based on information above, generate the correct natural language response and JSON output\n",
        "\n",
        "    if  **Current Conversation History**:\n",
        "    {conversation_history}\n",
        "\n",
        "    and **Customer's New Input:** \"{user_input}\".\n",
        "\n",
        "    \"\"\"\n",
        "    return base_prompt\n",
        "\n",
        "# Function to get response from the model\n",
        "def get_model_response(prompt):\n",
        "    response = model(prompt, max_new_tokens=300, truncation=True)[0]['generated_text']\n",
        "\n",
        "    # Parsing to get the expected response parts\n",
        "    try:\n",
        "        natural_response, json_response = response.split(\"JSON Output:\", 1)\n",
        "    except ValueError:\n",
        "        natural_response, json_response = \"Error: Invalid response format\", \"{}\"\n",
        "    return natural_response.strip(), json_response.strip()\n",
        "\n",
        "# Interaction loop to keep track of orders\n",
        "while True:\n",
        "    user_input = input(\"Customer: \")\n",
        "\n",
        "    # Generate prompt with conversation history\n",
        "    prompt = generate_prompt(conversation_history, user_input)\n",
        "\n",
        "    # Get the response from the model\n",
        "    natural_response, json_response = get_model_response(prompt)\n",
        "\n",
        "    # Update conversation history with the customer's input and the model's response\n",
        "    conversation_history += f\"\\nCustomer's {input_counter} Input: {user_input}\"\n",
        "    input_counter += 1  # Increment counter for the next input\n",
        "\n",
        "    # Print current response\n",
        "    print(f\"Model Response: {natural_response}\")\n",
        "    print(f\"JSON Output: {json_response}\")\n",
        "\n",
        "    # Break the loop if the customer confirms the order\n",
        "    if \"주문 완료\" in user_input:\n",
        "        print(\"Order confirmed. Thank you!\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457,
          "referenced_widgets": [
            "37171c5ae52345feb48ef68c1fb7974e",
            "9957a672ede0454ab2cdc00f58c210ed",
            "c4677f58743b477d8521b6d0f1ba42b0",
            "ec1d27d4169a42d6a4e7cd7fd62eeee2",
            "d926c90efab74a5d8e481cd46b675209",
            "28c5a6c0cf824eb3aa1bf58e7733521e",
            "70e456b1ca074b2f86acb5ed1d5bb6f8",
            "ecfffa733f5142bc9c5c05e542fdcbac",
            "d41377639ed0439195e19f85c6397522",
            "3b573bc395e4473b90ebdaa829fb83ad",
            "013a471bc660416f8d5d04abd8ab84af"
          ]
        },
        "id": "xEYW1GEhK6e5",
        "outputId": "a6b4a735-cf6c-4d39-eb08-52b8153f6170"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37171c5ae52345feb48ef68c1fb7974e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 25.06 MiB is free. Process 6068 has 14.72 GiB memory in use. Of the allocated memory 14.62 GiB is allocated by PyTorch, and 1.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6c5337a43720>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Initialize model and conversation history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text-generation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bllossom/llama-3.2-Korean-Bllossom-3B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mconversation_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minput_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# Start counter for numbering inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processor\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpipeline_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         self.check_model_type(\n\u001b[1;32m     98\u001b[0m             \u001b[0mTF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mMODEL_FOR_CAUSAL_LM_MAPPING_NAMES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mhf_device_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m         ):\n\u001b[0;32m--> 935\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;31m# If the model can generate, create a local generation config. This is done to avoid side-effects on the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3155\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3156\u001b[0m                 )\n\u001b[0;32m-> 3157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 25.06 MiB is free. Process 6068 has 14.72 GiB memory in use. Of the allocated memory 14.62 GiB is allocated by PyTorch, and 1.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize model and conversation history\n",
        "model = pipeline('text-generation', model='Bllossom/llama-3.2-Korean-Bllossom-3B', device=0)\n",
        "conversation_history = \"\"\n",
        "input_counter = 1  # Start counter for numbering inputs\n",
        "\n",
        "# Function to generate prompt based on current input and conversation history\n",
        "def generate_prompt(conversation_history, user_input):\n",
        "    base_prompt = f\"\"\"\n",
        "    당신은 가상 커피 키오스크를 운영하고 있으며, 고객이 음성 인식(STT)으로 커피 주문을 제공합니다. 당신의 역할은 이 입력을 이해하고 처리하여 자연스럽게 한국어로 응답하며, 백엔드 처리를 위한 구조화된 JSON 파일을 생성하는 것입니다.\n",
        "\n",
        "    **핵심 요구사항:**\n",
        "    - **메뉴 항목:** 키오스크는 다음과 같은 음료를 제공합니다: 아메리카노, 라떼, 카푸치노, 카페모카, 바닐라라떼, 에스프레소, 카라멜마끼아또, 허브티, 홍차, 초콜릿라떼, 레몬에이드, 복숭아아이스티, 딸기스무디, 망고스무디, 키위주스, 토마토주스.\n",
        "    - **기본 값**: 고객이 크기 또는 온도를 명시하지 않은 경우에만 \"미디움\" 크기와 \"핫\" 온도를 기본으로 사용하세요.\n",
        "    - **추측하지 않기**: 고객이 온도나 크기 등을 명시할 경우, 기본값을 사용하지 않고 그대로 반영하십시오. 예를 들어:\n",
        "        - 입력: \"아이스 라떼 두잔 주세요\"는 \"아이스 라떼\"로 해석되어야 하며, 핫 라떼로 변경하지 마세요.\n",
        "         **예시**:\n",
        "\n",
        "    - 고객 입력: \"아이스 라떼 두잔 주세요.\"\n",
        "    - 올바른 자연어 응답: \"아이스 라떼 2잔 주문되었습니다. 주문하신 내용은 다음과 같습니다:\n",
        "    - 라떼 2잔(아이스, 미디움)\"\n",
        "    - 올바른 JSON 출력:\n",
        "      {{\n",
        "          \"action\": \"create_order\",\n",
        "          \"order_items\": [\n",
        "              {{\n",
        "                  \"drink\": \"라떼\",\n",
        "                  \"size\": \"미디움\",\n",
        "                  \"temperature\": \"아이스\",\n",
        "                  \"quantity\": 2,\n",
        "                  \"add_ons\": [],\n",
        "                  \"extra_shots\": 0\n",
        "              }}\n",
        "          ]\n",
        "      }}\n",
        "\n",
        "    **연속된 주문 처리**: 한 고객의 연속된 주문으로 간주하고 \"주문 완료할게\"라는 명시적 확인이 없으면 이전 주문을 유지하세요. \"주문 완료할게\"라는 입력이 있을 경우 해당 고객의 주문이 완료된 것으로 간주하고, 다음 고객을 위해 주문 기록을 초기화하세요.\n",
        "\n",
        "    **응답 지침**:\n",
        "    - **주문 연속성**: 각 새로운 입력은 동일한 고객의 지속적인 주문의 일부로 간주됩니다. 기존 주문 기록을 유지하며 \"주문하신 내용은 다음과 같습니다:\"로 모든 주문 항목을 요약합니다.\n",
        "    - **완료 확인**: 고객이 \"주문 완료할게\"라고 말할 때만 주문을 최종화하고 마감합니다. 그 전까지는 추가나 수정된 항목을 포함하여 요약을 갱신합니다.\n",
        "    - **자연어 응답**: 각 동작을 명확한 요약 형식으로 확인합니다:\n",
        "        - [음료] [수량] 주문되었습니다.\n",
        "        - \"주문하신 내용은 다음과 같습니다:\"로 시작하여 전체 주문 항목을 나열하세요.\n",
        "    - **JSON 출력**: 최신 입력에 해당하는 항목만 JSON에 반영하세요. 고객이 완료를 확인하면 JSON 출력을 초기화하세요.\n",
        "\n",
        "    **\"주문 완료할게\" 입력 시**:\n",
        "    - **자연어 응답**: \"주문이 완료되었습니다. 결제는 카드리더기를 사용해주세요. 감사합니다.\"\n",
        "    - **JSON 출력**: 다음 세션을 위해 초기화합니다.\n",
        "\n",
        "    **추가 사항**:\n",
        "        - **잘못된 입력이나 약어**: 고객이 음료 이름의 약어(예: 아아 = 아이스 아메리카노, 뜨아 = 핫 아메리카노)나 철자가 틀린 표현을 사용한 경우 이를 올바르게 이해하고 해석하세요.\n",
        "        - **메뉴에 없는 항목**: 메뉴에 없는 항목을 요청한 경우, 해당 항목이 없다고 안내하고 대체 가능한 메뉴를 제안합니다. 잘못된 항목에 대해 JSON 파일을 생성하지 마세요.\n",
        "        - **여러 요청**: 고객이 한 번에 여러 요청을 할 수 있으므로 고객이 주문 완료를 확인할 때까지 각 단계를 기억하고 처리하세요.\n",
        "        - **고객 확인**: 고객이 주문 완료를 확인하기 전까지는 요청을 계속해서 기억하고 추가해 나갑니다.\n",
        "\n",
        "    **예시 시나리오**:\n",
        "    1. **주문 생성**:\n",
        "      - **고객 입력**: \"아메리카노 4잔 주세요.\"\n",
        "      - **자연어 응답**: \"아메리카노 4잔 주문되었습니다. 주문하신 내용은 다음과 같습니다:\n",
        "      - 아메리카노 4잔(핫, 미디움)\"\n",
        "      - **JSON 출력**:\n",
        "        ```json\n",
        "        {{\n",
        "          \"action\": \"create_order\",\n",
        "          \"order_items\": [\n",
        "            {{\n",
        "              \"drink\": \"아메리카노\",\n",
        "              \"size\": \"미디움\",\n",
        "              \"temperature\": \"핫\",\n",
        "              \"quantity\": 4,\n",
        "              \"add_ons\": [],\n",
        "              \"extra_shots\": 0\n",
        "            }}\n",
        "          ]\n",
        "        }}\n",
        "        ```\n",
        "\n",
        "    2. **주문 요약 요청**:\n",
        "      - **고객 입력**: \"내가 지금까지 뭘 주문했지?\"\n",
        "      - **자연어 응답**: \"지금까지의 주문내역입니다:\n",
        "      - 아메리카노 4잔(핫, 미디움)\n",
        "      - 카페라떼 라지 2잔 (핫, 라지)\"\n",
        "      - **JSON 출력**:\n",
        "        ```json\n",
        "        {{\n",
        "          \"action\": \"list_orders\",\n",
        "          \"order_list\": [\n",
        "            {{\n",
        "              \"drink\": \"아메리카노\",\n",
        "              \"size\": \"미디움\",\n",
        "              \"temperature\": \"핫\",\n",
        "              \"quantity\": 4,\n",
        "              \"add_ons\": [],\n",
        "              \"extra_shots\": 0\n",
        "            }},\n",
        "            {{\n",
        "              \"drink\": \"카페라떼\",\n",
        "              \"size\": \"라지\",\n",
        "              \"temperature\": \"핫\",\n",
        "              \"quantity\": 2,\n",
        "              \"add_ons\": [],\n",
        "              \"extra_shots\": 0\n",
        "            }}\n",
        "          ]\n",
        "        }}\n",
        "        ```\n",
        "\n",
        "    3. **옵션 추가**:\n",
        "      - **고객 입력**: \"휘핑크림 추가 해주세요.\"\n",
        "      - **자연어 응답**: \"휘핑크림이 추가되었습니다. 주문하신 내용은 다음과 같습니다:\n",
        "      - 라떼 1잔(핫, 미디움, 휘핑크림 추가)\"\n",
        "      - **JSON 출력**:\n",
        "        ```json\n",
        "        {{\n",
        "          \"action\": \"modify_order\",\n",
        "          \"drink\": \"아메리카노\",\n",
        "          \"size\": \"미디움\",\n",
        "          \"temperature\": \"핫\",\n",
        "          \"quantity\": 1,\n",
        "          \"add_ons\": [\"휘핑크림\"],\n",
        "          \"extra_shots\": 0\n",
        "        }}\n",
        "        ```\n",
        "\n",
        "    4. **기존 주문 수정**:\n",
        "      - **고객 입력**: \"주문한거 아이스 라떼로 바꿔줘.\"\n",
        "      - **자연어 응답**: \"주문이 아메리카노에서 아이스 라떼로 변경되었습니다. 주문하신 내용은 다음과 같습니다:\n",
        "      - 라떼 1잔 (아이스, 미디움)\"\n",
        "      - **JSON 출력**:\n",
        "        ```json\n",
        "        {{\n",
        "          \"action\": \"modify_order\",\n",
        "          \"old_drink\": \"아메리카노\",\n",
        "          \"new_drink\": \"라떼\",\n",
        "          \"size\": \"미디움\",\n",
        "          \"temperature\": \"아이스\",\n",
        "          \"quantity\": 1,\n",
        "          \"add_ons\": [],\n",
        "          \"extra_shots\": 0\n",
        "        }}\n",
        "        ```\n",
        "\n",
        "    5. **약어나 철자 오류 처리**:\n",
        "      - **고객 입력**: \"아아 한 잔 주세요.\"\n",
        "      - **자연어 응답**: \"아이스 아메리카노 한 잔 주문되었습니다. 주문하신 내용은 다음과 같습니다:\n",
        "      - 아메리카노 1잔 (아이스, 미디움)\"\n",
        "      - **JSON 출력**:\n",
        "        ```json\n",
        "        {{\n",
        "          \"action\": \"create_order\",\n",
        "          \"order_items\": [\n",
        "            {{\n",
        "              \"drink\": \"아메리카노\",\n",
        "              \"size\": \"미디움\",\n",
        "              \"temperature\": \"아이스\",\n",
        "              \"quantity\": 1,\n",
        "              \"add_ons\": [],\n",
        "              \"extra_shots\": 0\n",
        "            }}\n",
        "          ]\n",
        "        }}\n",
        "        ```\n",
        "\n",
        "    6. **메뉴에 없는 항목 요청 처리**:\n",
        "      - **고객 입력**: \"초코라떼 주세요.\"\n",
        "      - **자연어 응답**: \"죄송합니다, 초코라떼는 메뉴에 없습니다. 대신 초콜릿라떼를 추천드립니다.\"\n",
        "      - **JSON 출력**:\n",
        "        ```json\n",
        "        {{\n",
        "          \"action\": \"recommend_closest_item\",\n",
        "          \"requested_item\": \"초코라떼\",\n",
        "          \"recommended_item\": \"초콜릿라떼\"\n",
        "        }}\n",
        "        ```\n",
        "\n",
        "    7. **주문 완료 확인**:\n",
        "      - **고객 입력**: \"주문 완료할게요.\"\n",
        "      - **자연어 응답**: \"주문이 완료되었습니다. 결제는 카드리더기를 사용해주세요. 감사합니다.\"\n",
        "      - **JSON 출력**:\n",
        "        ```json\n",
        "        {{\n",
        "          \"action\": \"complete_order\",\n",
        "          \"order_items\": [\n",
        "            {{\n",
        "              \"drink\": \"아메리카노\",\n",
        "              \"size\": \"미디움\",\n",
        "              \"temperature\": \"핫\",\n",
        "              \"quantity\": 1,\n",
        "              \"add_ons\": [],\n",
        "              \"extra_shots\": 0\n",
        "            }}\n",
        "          ]\n",
        "        }}\n",
        "        ```\n",
        "\n",
        "    **기존 대화 내역**:\n",
        "    {conversation_history}\n",
        "\n",
        "    **고객의 새 입력:** \"{user_input}\"\n",
        "\n",
        "    위 정보를 기반으로 자연어 응답과 JSON 출력을 생성하세요.\n",
        "    \"\"\"\n",
        "\n",
        "    return base_prompt\n",
        "\n",
        "\n",
        "\n",
        "# Interaction loop to keep track of orders\n",
        "while True:\n",
        "    user_input = input(\"Customer: \")\n",
        "\n",
        "    # Generate prompt with conversation history\n",
        "    prompt = generate_prompt(conversation_history, user_input)\n",
        "\n",
        "    # Get the response from the model\n",
        "    response = model(prompt, max_new_tokens=300, truncation=True)[0]['generated_text']\n",
        "\n",
        "    # Update conversation history with the customer's input and the model's response\n",
        "    conversation_history += f\"\\n같은 고객의 {input_counter}번째 요청: {user_input}\"\n",
        "    input_counter += 1  # Increment counter for the next input\n",
        "\n",
        "    # Print current response\n",
        "    print(f\"Model Response: {response}\")\n",
        "\n",
        "\n",
        "    # Break the loop if the customer confirms the order\n",
        "    if \"주문 완료\" in user_input:\n",
        "        print(\"Order confirmed. Thank you!\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import json\n",
        "\n",
        "# Initialize the Korean model for text generation\n",
        "model = pipeline('text-generation', model='Bllossom/llama-3.2-Korean-Bllossom-3B', device=0)\n",
        "\n",
        "# Initialize conversation history, summary history, and order confirmation flag\n",
        "conversation_history = []  # Stores each customer input as a string entry in the format: \"Customer's X Input: [input]\"\n",
        "summary_history = []       # Stores each cumulative summary of orders\n",
        "order_confirmed = False    # Tracks if the order is finalized\n",
        "\n",
        "# Function to generate prompt based on conversation history and new input\n",
        "def generate_prompt(conversation_history, user_input):\n",
        "    # Format the conversation history and cumulative summary as a single string\n",
        "    formatted_history = \"\\n\".join(conversation_history) if conversation_history else \"none\"\n",
        "    formatted_summary = \"\\n\".join(summary_history) if summary_history else \"none\"\n",
        "\n",
        "    base_prompt = f\"\"\"\n",
        "    You are operating a virtual coffee kiosk that receives speech-to-text (STT) inputs from customers placing coffee orders. Your role is to understand and process these inputs, respond naturally in Korean, and generate a structured JSON file with the correct details for backend processing.\n",
        "\n",
        "    **Key Requirements**:\n",
        "    - **Menu Items**: The kiosk offers the following drinks:\n",
        "        - Hot and Iced Coffee: 아메리카노, 라떼, 카푸치노, 카페모카, 바닐라라떼, 에스프레소, 카라멜마끼아또\n",
        "        - Tea: 허브티, 홍차\n",
        "        - Specialty Drinks: 초콜릿라떼, 레몬에이드, 복숭아아이스티, 딸기스무디, 망고스무디, 키위주스, 토마토주스\n",
        "    - **Default Values**:\n",
        "        - Use default size \"미디움\" and temperature \"핫\" only if the customer does not specify these details.\n",
        "    - **Do Not Make Assumptions**:\n",
        "        - If the customer specifies temperature or size, do not override it with defaults. For instance, if they say \"아이스 라떼 두잔 주세요\", the output should indicate \"아이스\" without changing it to \"핫\".\n",
        "\n",
        "    **Customer Input and Expected Output Format**:\n",
        "    - Each response should have:\n",
        "      1. **Natural Language Confirmation**: Respond in Korean, starting with an action confirmation such as \"[Drink] [quantity] 주문되었습니다.\" and follow with a full summary of all items ordered so far, beginning with \"지금까지 주문하신 내용은 다음과 같습니다:\".(also if there are any instance in history it should be added after this sentence)\n",
        "      2. **Structured JSON Output**: Each JSON output should only contain the items directly requested in the latest input, not a full history.\n",
        "\n",
        "    **JSON Output Format**:\n",
        "    - The JSON should be structured as follows:\n",
        "      ```json\n",
        "      {{\n",
        "          \"action\": \"[action_type]\",\n",
        "          \"order_items\": [\n",
        "              {{\n",
        "                  \"drink\": \"[Drink Name]\",\n",
        "                  \"size\": \"[Size]\",\n",
        "                  \"temperature\": \"[Temperature]\",\n",
        "                  \"quantity\": [Quantity],\n",
        "                  \"add_ons\": [List of add-ons if any],\n",
        "                  \"extra_shots\": [Number of extra shots if any]\n",
        "              }}\n",
        "          ]\n",
        "      }}\n",
        "      ```\n",
        "      - **Example JSON Output**:\n",
        "        ```json\n",
        "        {{\n",
        "            \"action\": \"create_order\",\n",
        "            \"order_items\": [\n",
        "                {{\n",
        "                    \"drink\": \"아메리카노\",\n",
        "                    \"size\": \"미디움\",\n",
        "                    \"temperature\": \"핫\",\n",
        "                    \"quantity\": 1,\n",
        "                    \"add_ons\": [],\n",
        "                    \"extra_shots\": 0\n",
        "                }}\n",
        "            ]\n",
        "        }}\n",
        "        ```\n",
        "\n",
        "    **Available Actions for JSON Output**:\n",
        "    - **create_order**: For new drink orders.\n",
        "    - **add_item**: For adding a new item to the current order.\n",
        "    - **modify_order**: For changing an existing item (e.g., modifying size or temperature).\n",
        "    - **cancel_order**: To remove an order item or reset the order.\n",
        "    - **recommend_closest_item**: If a requested item is unavailable, recommend the closest item.\n",
        "    - **show_order_summary**: Display a summary of all items ordered so far.\n",
        "    - **complete_order**: Finalize the order after confirmation.\n",
        "\n",
        "    **Specific Scenarios and Expected Outputs**:\n",
        "    - **Creating a New Order**:\n",
        "      - **Customer Input**: \"아메리카노 4잔 주세요.\"\n",
        "      - **Natural Language Response**: \"아메리카노 4잔 주문되었습니다. 지금까지 주문하신 내용은 다음과 같습니다:\n",
        "      -아메리카노 4잔 (핫, 미디움)\"\n",
        "      - **JSON Output**:\n",
        "        ```json\n",
        "        {{\n",
        "          \"action\": \"create_order\",\n",
        "          \"order_items\": [\n",
        "            {{\n",
        "              \"drink\": \"아메리카노\",\n",
        "              \"size\": \"미디움\",\n",
        "              \"temperature\": \"핫\",\n",
        "              \"quantity\": 4,\n",
        "              \"add_ons\": [],\n",
        "              \"extra_shots\": 0\n",
        "            }}\n",
        "          ]\n",
        "        }}\n",
        "        ```\n",
        "\n",
        "    - **Requesting Order Summary**:\n",
        "      - **Customer Input**: \"내가 지금까지 뭘 주문했지?\"\n",
        "      - **Natural Language Response**: \"지금까지 주문하신 내용은 다음과 같습니다:\n",
        "      -아메리카노 4잔(핫, 미디움)\n",
        "      -카페라떼 라지 2잔 (핫, 라지)\"\n",
        "      - **JSON Output**: None (as it is just a summary request without any new action).\n",
        "\n",
        "    - **Modifying an Existing Order**:\n",
        "      - **Customer Input**: \"주문한거 아이스 라떼로 바꿔줘.\"\n",
        "      - **Natural Language Response**: \"주문이 아메리카노에서 아이스 라떼로 변경되었습니다. 주문하신 내용은 다음과 같습니다:\n",
        "      -라떼 1잔 (아이스, 미디움)\"\n",
        "      - **JSON Output**:\n",
        "        ```json\n",
        "        {{\n",
        "          \"action\": \"modify_order\",\n",
        "          \"old_drink\": \"아메리카노\",\n",
        "          \"new_drink\": \"라떼\",\n",
        "          \"size\": \"미디움\",\n",
        "          \"temperature\": \"아이스\",\n",
        "          \"quantity\": 1,\n",
        "          \"add_ons\": [],\n",
        "          \"extra_shots\": 0\n",
        "        }}\n",
        "        ```\n",
        "\n",
        "    - **Short Names or Misspellings**:\n",
        "      - Recognize common shorthand or misspellings. For example:\n",
        "        - \"아아\" should be interpreted as \"아이스 아메리카노\".\n",
        "        - \"뜨아\" should be interpreted as \"핫 아메리카노\".\n",
        "\n",
        "    - **Unavailable Items**:\n",
        "      - If the customer requests an item not on the menu, respond politely and recommend a similar item if available.\n",
        "      - **Example**:\n",
        "        - **Customer Input**: \"초코라떼 주세요.\"\n",
        "        - **Natural Language Response**: \"죄송합니다, 초코라떼는 메뉴에 없습니다. 대신 초콜릿라떼를 추천드립니다.\"\n",
        "        - **JSON Output**:\n",
        "          ```json\n",
        "          {{\n",
        "            \"action\": \"recommend_closest_item\",\n",
        "            \"requested_item\": \"초코라떼\",\n",
        "            \"recommended_item\": \"초콜릿라떼\"\n",
        "          }}\n",
        "          ```\n",
        "\n",
        "    - **Order Confirmation**:\n",
        "      - **Customer Input**: \"주문 완료할게요.\"\n",
        "      - **Natural Language Response**: \"주문이 완료되었습니다. 결제는 카드리더기를 사용해주세요. 감사합니다.\"\n",
        "      - **JSON Output**: Clear/reset for the next session.\n",
        "\n",
        "    **Response Rules**:\n",
        "    - Treat each new input as part of the same order until \"주문 완료할게\" is received, which finalizes the order.\n",
        "    - Always confirm the latest action first in the natural language response, followed by a full order summary.\n",
        "    - Ensure each JSON output reflects only the latest request, not the entire order history.\n",
        "\n",
        "    Based on this information,\n",
        "    (keep in mind that) Each JSON output should only contain the items directly requested in **Customer's New Input**, not a full history.\n",
        "    if **Current Conversation History**:\n",
        "    {formatted_history}\n",
        "\n",
        "    based on **Cumulative Order Summary So Far**:\n",
        "    {formatted_summary}\n",
        "\n",
        "    and **Customer's New Input**: \"{user_input}\"\n",
        "\n",
        "    generate the appropriate natural language response and JSON output\n",
        "    \"\"\"\n",
        "    return base_prompt.strip()\n",
        "\n",
        "# Function to get response from the Korean model pipeline\n",
        "def get_response(prompt):\n",
        "    response = model(prompt, max_new_tokens=150, num_return_sequences=1, temperature=0.1, top_p=0.9, truncation=True)\n",
        "    return response[0]['generated_text']\n",
        "\n",
        "# Function to parse response into natural language and JSON output\n",
        "def parse_response(response):\n",
        "    try:\n",
        "        natural_language_response, json_output = response.split(\"JSON Output:\")\n",
        "        json_data = json.loads(json_output.strip())\n",
        "        return natural_language_response.strip(), json_data\n",
        "    except ValueError as e:\n",
        "        print(\"Error parsing response:\", e)\n",
        "        return response, None\n",
        "\n",
        "# Main interaction loop\n",
        "print(\"Welcome to the virtual coffee kiosk! What would you like to order?\")\n",
        "input_counter = 1\n",
        "\n",
        "while not order_confirmed:\n",
        "    # Take user input\n",
        "    user_input = input(\"Customer: \")\n",
        "\n",
        "    # Check if customer confirms the order\n",
        "    if \"주문 완료할게\" in user_input:\n",
        "        order_confirmed = True\n",
        "        print(\"Kiosk: 주문이 완료되었습니다. 결제는 카드리더기를 사용해주세요. 감사합니다.\")\n",
        "        conversation_history.clear()\n",
        "        summary_history.clear()\n",
        "        continue\n",
        "\n",
        "    # Add the latest user input to the conversation history\n",
        "    conversation_history.append(f\"Customer's {input_counter} Input: {user_input}\")\n",
        "    input_counter += 1\n",
        "\n",
        "    # Generate the prompt based on the conversation history and new user input\n",
        "    prompt = generate_prompt(conversation_history, user_input)\n",
        "\n",
        "    # Get the model's response\n",
        "    response = get_response(prompt)\n",
        "\n",
        "    # Parse the model's response\n",
        "    natural_language_response, json_output = parse_response(response)\n",
        "\n",
        "    # Save natural language order summary to `summary_history`\n",
        "    if natural_language_response:\n",
        "        summary_line = natural_language_response.split(\"\\n\", 1)[1]  # Extracts summary part\n",
        "        summary_history.append(summary_line)\n",
        "\n",
        "    # Print the final natural language response for the customer\n",
        "    #print(\"Kiosk:\", natural_language_response)\n",
        "    # Print only the Natural Language Response and JSON Output in the required format\n",
        "    print(\"**Natural Language Response**:\")\n",
        "    print(natural_language_response)\n",
        "    print(\"\\n**JSON Output**:\")\n",
        "    print(json.dumps(json_output, indent=2, ensure_ascii=False))  # Pretty-print JSON output\n",
        "\n",
        "    # Special handling for order summary requests\n",
        "    if \"내가 지금까지 뭘 주문했지\" in user_input:\n",
        "        # Provide cumulative order summary as a natural response without JSON output\n",
        "        print(\"Kiosk:\", \"지금까지의 주문내역입니다:\\n\" + \"\\n\".join(summary_history))\n",
        "\n",
        "    # Handle order cancellation requests\n",
        "    if \"주문 취소\" in user_input:\n",
        "        # Clear conversation history and summary history for a reset\n",
        "        print(\"Kiosk: 주문이 취소되었습니다. 새 주문을 시작해 주세요.\")\n",
        "        conversation_history.clear()\n",
        "        summary_history.clear()\n",
        "        input_counter = 1\n",
        "\n",
        "# End of the session\n",
        "print(\"Thank you for using the coffee kiosk!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "16eb3eee308a4ddfb8f7bd4b1cde116b",
            "f17c74b235b04cbfa5096a2775e87754",
            "e25ade8b31cc480193ebee0dc1001666",
            "3e9884f8e2604c409d129c13907f3678",
            "a7802a4c303b4bcfbf373d8b7c8e9ba8",
            "6a095636d3c744d99860ec23ba43533a",
            "dd5acae3220d44f2956b7d45433cce89",
            "f87167e50fe04b6ead66927ec2626738",
            "1909d68550c748e286b93ff1d63e18a5",
            "0468152cfd0b4d56a1eac372b2a17163",
            "797ef2324e754da88a22d6bce47ef3bf"
          ]
        },
        "id": "QOj66ALUFeRl",
        "outputId": "9865d0c5-6ed7-4241-96a5-5b9e1162506c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-c24122568d05>\", line 1, in <cell line: 1>\n",
            "    from transformers import pipeline\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1766, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1778, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py\", line 26, in <module>\n",
            "    from ..image_processing_utils import BaseImageProcessor\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py\", line 21, in <module>\n",
            "    from .image_transforms import center_crop, normalize, rescale\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\", line 50, in <module>\n",
            "    import tensorflow as tf\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\", line 467, in <module>\n",
            "    importlib.import_module(\"keras.src.optimizers\")\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/__init__.py\", line 4, in <module>\n",
            "    from keras.api import DTypePolicy\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/api/__init__.py\", line 8, in <module>\n",
            "    from keras.api import activations\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/api/activations/__init__.py\", line 7, in <module>\n",
            "    from keras.src.activations import deserialize\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/__init__.py\", line 1, in <module>\n",
            "    from keras.src import activations\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/activations/__init__.py\", line 22, in <module>\n",
            "    from keras.src.saving import object_registration\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/saving/__init__.py\", line 7, in <module>\n",
            "    from keras.src.saving.saving_api import load_model\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\", line 7, in <module>\n",
            "    from keras.src.legacy.saving import legacy_h5_format\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\", line 13, in <module>\n",
            "    from keras.src.legacy.saving import saving_utils\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/legacy/saving/saving_utils.py\", line 10, in <module>\n",
            "    from keras.src import models\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/models/__init__.py\", line 1, in <module>\n",
            "    from keras.src.models.functional import Functional\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/models/functional.py\", line 16, in <module>\n",
            "    from keras.src.models.model import Model\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/models/model.py\", line 12, in <module>\n",
            "    from keras.src.trainers import trainer as base_trainer\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py\", line 14, in <module>\n",
            "    from keras.src.trainers.data_adapters import data_adapter_utils\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/__init__.py\", line 4, in <module>\n",
            "    from keras.src.trainers.data_adapters import array_data_adapter\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\", line 7, in <module>\n",
            "    from keras.src.trainers.data_adapters import array_slicing\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/array_slicing.py\", line 11, in <module>\n",
            "    import pandas\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/__init__.py\", line 26, in <module>\n",
            "    from pandas.compat import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/compat/__init__.py\", line 27, in <module>\n",
            "    from pandas.compat.pyarrow import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/compat/pyarrow.py\", line 8, in <module>\n",
            "    import pyarrow as pa\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyarrow/__init__.py\", line 65, in <module>\n",
            "    import pyarrow.lib as _lib\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-c24122568d05>\", line 1, in <cell line: 1>\n",
            "    from transformers import pipeline\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1766, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1778, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py\", line 26, in <module>\n",
            "    from ..image_processing_utils import BaseImageProcessor\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py\", line 21, in <module>\n",
            "    from .image_transforms import center_crop, normalize, rescale\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\", line 50, in <module>\n",
            "    import tensorflow as tf\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\", line 467, in <module>\n",
            "    importlib.import_module(\"keras.src.optimizers\")\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/__init__.py\", line 4, in <module>\n",
            "    from keras.api import DTypePolicy\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/api/__init__.py\", line 8, in <module>\n",
            "    from keras.api import activations\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/api/activations/__init__.py\", line 7, in <module>\n",
            "    from keras.src.activations import deserialize\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/__init__.py\", line 1, in <module>\n",
            "    from keras.src import activations\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/activations/__init__.py\", line 22, in <module>\n",
            "    from keras.src.saving import object_registration\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/saving/__init__.py\", line 7, in <module>\n",
            "    from keras.src.saving.saving_api import load_model\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\", line 7, in <module>\n",
            "    from keras.src.legacy.saving import legacy_h5_format\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\", line 13, in <module>\n",
            "    from keras.src.legacy.saving import saving_utils\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/legacy/saving/saving_utils.py\", line 10, in <module>\n",
            "    from keras.src import models\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/models/__init__.py\", line 1, in <module>\n",
            "    from keras.src.models.functional import Functional\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/models/functional.py\", line 16, in <module>\n",
            "    from keras.src.models.model import Model\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/models/model.py\", line 12, in <module>\n",
            "    from keras.src.trainers import trainer as base_trainer\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py\", line 14, in <module>\n",
            "    from keras.src.trainers.data_adapters import data_adapter_utils\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/__init__.py\", line 4, in <module>\n",
            "    from keras.src.trainers.data_adapters import array_data_adapter\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\", line 7, in <module>\n",
            "    from keras.src.trainers.data_adapters import array_slicing\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/array_slicing.py\", line 11, in <module>\n",
            "    import pandas\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/__init__.py\", line 49, in <module>\n",
            "    from pandas.core.api import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/api.py\", line 9, in <module>\n",
            "    from pandas.core.dtypes.dtypes import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/dtypes.py\", line 24, in <module>\n",
            "    from pandas._libs import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyarrow/__init__.py\", line 65, in <module>\n",
            "    import pyarrow.lib as _lib\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16eb3eee308a4ddfb8f7bd4b1cde116b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the virtual coffee kiosk! What would you like to order?\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b84dd9b4fcd84551b7d33567fbe2f368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_892ca30b4c984cf0aa8f54e27e5a1964",
              "IPY_MODEL_7c7566a7ac444b03b147f34794e45e23",
              "IPY_MODEL_bc71715f23c74b468f6df258607643db"
            ],
            "layout": "IPY_MODEL_b894d56fbecf4c9f9bddfc922b805547"
          }
        },
        "892ca30b4c984cf0aa8f54e27e5a1964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d373cc1b52a4c7c9994769b5a61536b",
            "placeholder": "​",
            "style": "IPY_MODEL_d1a2e5e27d0f492189146601b686396b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7c7566a7ac444b03b147f34794e45e23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93b2534ddbae4abeb267ffa50bf5bbe1",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e913456c33d245c4ac55e5734e83a066",
            "value": 2
          }
        },
        "bc71715f23c74b468f6df258607643db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3d639df31374eedabcfe07eb0b80a33",
            "placeholder": "​",
            "style": "IPY_MODEL_b4f30ec7416a4d74b9d1a199e26f5827",
            "value": " 2/2 [00:26&lt;00:00, 11.99s/it]"
          }
        },
        "b894d56fbecf4c9f9bddfc922b805547": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d373cc1b52a4c7c9994769b5a61536b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1a2e5e27d0f492189146601b686396b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93b2534ddbae4abeb267ffa50bf5bbe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e913456c33d245c4ac55e5734e83a066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3d639df31374eedabcfe07eb0b80a33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4f30ec7416a4d74b9d1a199e26f5827": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37171c5ae52345feb48ef68c1fb7974e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9957a672ede0454ab2cdc00f58c210ed",
              "IPY_MODEL_c4677f58743b477d8521b6d0f1ba42b0",
              "IPY_MODEL_ec1d27d4169a42d6a4e7cd7fd62eeee2"
            ],
            "layout": "IPY_MODEL_d926c90efab74a5d8e481cd46b675209"
          }
        },
        "9957a672ede0454ab2cdc00f58c210ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28c5a6c0cf824eb3aa1bf58e7733521e",
            "placeholder": "​",
            "style": "IPY_MODEL_70e456b1ca074b2f86acb5ed1d5bb6f8",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c4677f58743b477d8521b6d0f1ba42b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecfffa733f5142bc9c5c05e542fdcbac",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d41377639ed0439195e19f85c6397522",
            "value": 2
          }
        },
        "ec1d27d4169a42d6a4e7cd7fd62eeee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b573bc395e4473b90ebdaa829fb83ad",
            "placeholder": "​",
            "style": "IPY_MODEL_013a471bc660416f8d5d04abd8ab84af",
            "value": " 2/2 [00:26&lt;00:00, 11.93s/it]"
          }
        },
        "d926c90efab74a5d8e481cd46b675209": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28c5a6c0cf824eb3aa1bf58e7733521e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70e456b1ca074b2f86acb5ed1d5bb6f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecfffa733f5142bc9c5c05e542fdcbac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d41377639ed0439195e19f85c6397522": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b573bc395e4473b90ebdaa829fb83ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "013a471bc660416f8d5d04abd8ab84af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16eb3eee308a4ddfb8f7bd4b1cde116b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f17c74b235b04cbfa5096a2775e87754",
              "IPY_MODEL_e25ade8b31cc480193ebee0dc1001666",
              "IPY_MODEL_3e9884f8e2604c409d129c13907f3678"
            ],
            "layout": "IPY_MODEL_a7802a4c303b4bcfbf373d8b7c8e9ba8"
          }
        },
        "f17c74b235b04cbfa5096a2775e87754": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a095636d3c744d99860ec23ba43533a",
            "placeholder": "​",
            "style": "IPY_MODEL_dd5acae3220d44f2956b7d45433cce89",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e25ade8b31cc480193ebee0dc1001666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f87167e50fe04b6ead66927ec2626738",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1909d68550c748e286b93ff1d63e18a5",
            "value": 2
          }
        },
        "3e9884f8e2604c409d129c13907f3678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0468152cfd0b4d56a1eac372b2a17163",
            "placeholder": "​",
            "style": "IPY_MODEL_797ef2324e754da88a22d6bce47ef3bf",
            "value": " 2/2 [00:04&lt;00:00,  1.96s/it]"
          }
        },
        "a7802a4c303b4bcfbf373d8b7c8e9ba8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a095636d3c744d99860ec23ba43533a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd5acae3220d44f2956b7d45433cce89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f87167e50fe04b6ead66927ec2626738": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1909d68550c748e286b93ff1d63e18a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0468152cfd0b4d56a1eac372b2a17163": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "797ef2324e754da88a22d6bce47ef3bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}