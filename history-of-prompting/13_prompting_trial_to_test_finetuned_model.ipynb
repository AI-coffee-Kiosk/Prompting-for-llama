{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "1X_AGC8VedUt",
        "outputId": "69c88cc3-8c55-4089-b974-e365d981a535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735,
          "referenced_widgets": [
            "f838f19d08f040deaa9c0207612218af",
            "be4852d0f365488e96cbd0dbaa58325a",
            "8652c27ad5b0453f92c6065a1b1dc275",
            "0af4cd9f0fb84801b9be6b6ccf6bba0c",
            "e3afd7bbb16e4d8aa2fe9640af747cbb",
            "e0efcd5cfcc44915a15ade2c2ac00a62",
            "8da10b38669341ed94b930c2aacb1cb0",
            "d47ef1e7e8204cfa900fa7c7dda6b8e1",
            "c63d6757b3274801beafffa900af4b0d",
            "b443a35906ee4836be58190eb9717b4e",
            "5b5f1050f5274e2c914a9920de9ee198"
          ]
        },
        "id": "eiaTyDDmPDY5",
        "outputId": "6101c038-78f2-4e1c-b4a7-d6afd258d598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f838f19d08f040deaa9c0207612218af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the virtual coffee kiosk! What would you like to order?\n",
            "Error parsing response: not enough values to unpack (expected 2, got 1)\n",
            "Kiosk: .\n",
            "<|endoftext|>**Menu Items**: The kiosk offers the following drinks:\n",
            "- Hot and Iced Coffee: 아메리카노, 라떼, 카푸치노, 카페모카, 바닐라라떼, 에스프레소, 카라멜마끼아또\n",
            "- Tea: 허브티, 홍차\n",
            "- Specialty Drinks: 초콜릿라떼, 레몬에이드, 복숭아아이스티, 딸기스무디, 망고스무디, 키위주스, 토마토주스\n",
            "**Default Values**:\n",
            "- Use default size \"미디움\" and temperature \"핫\" only if the customer\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-913c38a28e5b>\u001b[0m in \u001b[0;36m<cell line: 188>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0morder_confirmed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;31m# Take user input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Customer: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;31m# Check if customer confirms the order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import json\n",
        "\n",
        "# Initialize the Korean model for text generation\n",
        "model = pipeline('text-generation', model='wolf010/5TH_fine_tuned_llama-3.2-Korean-Bllossom-3B', device=0)\n",
        "\n",
        "# Initialize conversation history, summary history, and order confirmation flag\n",
        "conversation_history = []  # Stores each customer input as a string entry in the format: \"Customer's X Input: [input]\"\n",
        "summary_history = []       # Stores each cumulative summary of orders\n",
        "order_confirmed = False    # Tracks if the order is finalized\n",
        "\n",
        "# Function to generate prompt based on conversation history and new input\n",
        "def generate_prompt(conversation_history):\n",
        "    # Format the conversation history and cumulative summary as a single string\n",
        "    if conversation_history:\n",
        "        formatted_history = \" \".join(conversation_history)\n",
        "    else:\n",
        "        formatted_history = \"none\"\n",
        "\n",
        "    base_prompt = f\"\"\"\n",
        "    You are operating a virtual coffee kiosk that receives speech-to-text (STT) inputs from customers placing coffee orders. Your role is to understand and process these inputs, respond naturally in Korean, and generate a structured JSON file with the correct details for backend processing.\n",
        "\n",
        "    **Key Requirements**:\n",
        "    - **Menu Items**: The kiosk offers the following drinks:\n",
        "        - Hot and Iced Coffee: 아메리카노, 라떼, 카푸치노, 카페모카, 바닐라라떼, 에스프레소, 카라멜마끼아또\n",
        "        - Tea: 허브티, 홍차\n",
        "        - Specialty Drinks: 초콜릿라떼, 레몬에이드, 복숭아아이스티, 딸기스무디, 망고스무디, 키위주스, 토마토주스\n",
        "    - **Default Values**:\n",
        "        - Use default size \"미디움\" and temperature \"핫\" only if the customer does not specify these details.\n",
        "    - **Do Not Make Assumptions**:\n",
        "        - If the customer specifies temperature or size, do not override it with defaults. For instance, if they say \"아이스 라떼 두잔 주세요\", the output should indicate \"아이스\" without changing it to \"핫\".\n",
        "    - **Current Conversation History** is a single-line cumulative log of all customer requests so far in this session. starting from 1\n",
        "    **Customer Input and Expected Output Format**:\n",
        "    - Each response should have:\n",
        "      1. **Natural Language Confirmation**: Respond in Korean, starting with an action confirmation such as \"[Drink] [quantity] 주문되었습니다.\" and follow with a full summary of all items ordered so far, beginning with \"지금까지 주문하신 내용은 다음과 같습니다:\".(also if there are any instance in history it should be added after this sentence)\n",
        "      2. **Structured JSON Output**: Each JSON output should only contain the items directly requested in the latest input, not a full history.\n",
        "\n",
        "    **JSON Output Format**:\n",
        "    - The JSON should be structured as follows:\n",
        "      ```json\n",
        "      {{\n",
        "          \"action\": \"[action_type]\",\n",
        "          \"order_items\": [\n",
        "              {{\n",
        "                  \"drink\": \"[Drink Name]\",\n",
        "                  \"size\": \"[Size]\",\n",
        "                  \"temperature\": \"[Temperature]\",\n",
        "                  \"quantity\": [Quantity],\n",
        "                  \"add_ons\": [List of add-ons if any],\n",
        "                  \"extra_shots\": [Number of extra shots if any]\n",
        "              }}\n",
        "          ]\n",
        "      }}\n",
        "      ```\n",
        "      - **Example JSON Output**:\n",
        "        ```json\n",
        "        {{\n",
        "            \"action\": \"create_order\",\n",
        "            \"order_items\": [\n",
        "                {{\n",
        "                    \"drink\": \"아메리카노\",\n",
        "                    \"size\": \"미디움\",\n",
        "                    \"temperature\": \"핫\",\n",
        "                    \"quantity\": 1,\n",
        "                    \"add_ons\": [],\n",
        "                    \"extra_shots\": 0\n",
        "                }}\n",
        "            ]\n",
        "        }}\n",
        "        ```\n",
        "\n",
        "    **Available Actions for JSON Output**:\n",
        "    - **create_order**: For new drink orders.\n",
        "    - **add_item**: For adding a new item to the current order.\n",
        "    - **modify_order**: For changing an existing item (e.g., modifying size or temperature).\n",
        "    - **cancel_order**: To remove an order item or reset the order.\n",
        "    - **recommend_closest_item**: If a requested item is unavailable, recommend the closest item.\n",
        "    - **show_order_summary**: Display a summary of all items ordered so far.\n",
        "    - **complete_order**: Finalize the order after confirmation.\n",
        "\n",
        "    **Specific Scenarios and Expected Outputs**:\n",
        "    - **Creating a New Order**:\n",
        "      - **Customer Input**: \"아메리카노 4잔 주세요.\"\n",
        "      - **Natural Language Response**: \"아메리카노 4잔 주문되었습니다. 지금까지 주문하신 내용은 다음과 같습니다:\n",
        "      -아메리카노 4잔 (핫, 미디움)\"\n",
        "      - **JSON Output**:\n",
        "        ```json\n",
        "        {{\n",
        "          \"action\": \"create_order\",\n",
        "          \"order_items\": [\n",
        "            {{\n",
        "              \"drink\": \"아메리카노\",\n",
        "              \"size\": \"미디움\",\n",
        "              \"temperature\": \"핫\",\n",
        "              \"quantity\": 4,\n",
        "              \"add_ons\": [],\n",
        "              \"extra_shots\": 0\n",
        "            }}\n",
        "          ]\n",
        "        }}\n",
        "        ```\n",
        "\n",
        "    - **Requesting Order Summary**:\n",
        "      - **Customer Input**: \"내가 지금까지 뭘 주문했지?\"\n",
        "      - **Natural Language Response**: \"지금까지 주문하신 내용은 다음과 같습니다:\n",
        "      -아메리카노 4잔(핫, 미디움)\n",
        "      -카페라떼 라지 2잔 (핫, 라지)\"\n",
        "      - **JSON Output**: None (as it is just a summary request without any new action).\n",
        "\n",
        "    - **Modifying an Existing Order**:\n",
        "      - **Customer Input**: \"주문한거 아이스 라떼로 바꿔줘.\"\n",
        "      - **Natural Language Response**: \"주문이 아메리카노에서 아이스 라떼로 변경되었습니다. 주문하신 내용은 다음과 같습니다:\n",
        "      -라떼 1잔 (아이스, 미디움)\"\n",
        "      - **JSON Output**:\n",
        "        ```json\n",
        "        {{\n",
        "          \"action\": \"modify_order\",\n",
        "          \"old_drink\": \"아메리카노\",\n",
        "          \"new_drink\": \"라떼\",\n",
        "          \"size\": \"미디움\",\n",
        "          \"temperature\": \"아이스\",\n",
        "          \"quantity\": 1,\n",
        "          \"add_ons\": [],\n",
        "          \"extra_shots\": 0\n",
        "        }}\n",
        "        ```\n",
        "\n",
        "    - **Short Names or Misspellings**:\n",
        "      - Recognize common shorthand or misspellings. For example:\n",
        "        - \"아아\" should be interpreted as \"아이스 아메리카노\".\n",
        "        - \"뜨아\" should be interpreted as \"핫 아메리카노\".\n",
        "\n",
        "    - **Unavailable Items**:\n",
        "      - If the customer requests an item not on the menu, respond politely and recommend a similar item if available.\n",
        "      - **Example**:\n",
        "        - **Customer Input**: \"초코라떼 주세요.\"\n",
        "        - **Natural Language Response**: \"죄송합니다, 초코라떼는 메뉴에 없습니다. 대신 초콜릿라떼를 추천드립니다.\"\n",
        "        - **JSON Output**:\n",
        "          ```json\n",
        "          {{\n",
        "            \"action\": \"recommend_closest_item\",\n",
        "            \"requested_item\": \"초코라떼\",\n",
        "            \"recommended_item\": \"초콜릿라떼\"\n",
        "          }}\n",
        "          ```\n",
        "\n",
        "    - **Order Confirmation**:\n",
        "      - **Customer Input**: \"주문 완료할게요.\"\n",
        "      - **Natural Language Response**: \"주문이 완료되었습니다. 결제는 카드리더기를 사용해주세요. 감사합니다.\"\n",
        "      - **JSON Output**: Clear/reset for the next session.\n",
        "\n",
        "    **Response Rules**:\n",
        "    - Treat each new input as part of the same order until \"주문 완료할게\" is received, which finalizes the order.\n",
        "    - Always confirm the latest action first in the natural language response, followed by a full order summary.\n",
        "    - Ensure each JSON output reflects only the latest request, not the entire order history.\n",
        "\n",
        "    Based on this information,\n",
        "    (keep in mind that) Each JSON output should only contain the items directly requested in **Customer's New Input**, not a full history.\n",
        "    if **Current Conversation History**:\n",
        "    {formatted_history}\n",
        "    generate the appropriate natural language response and JSON output\n",
        "    \"\"\"\n",
        "    return base_prompt.strip()\n",
        "\n",
        "\n",
        "# Function to get response from the Korean model pipeline\n",
        "def get_response(prompt):\n",
        "    response = model(prompt, max_new_tokens=150, num_return_sequences=1, temperature=0.1, top_p=0.9, truncation=False)\n",
        "    # Extract only the generated text, ignoring the prompt.\n",
        "    generated_text = response[0]['generated_text']\n",
        "    generated_text = generated_text[len(prompt):].strip()  # Remove the prompt part\n",
        "    return generated_text\n",
        "\n",
        "# Function to parse response into natural language and JSON output\n",
        "def parse_response(response):\n",
        "    try:\n",
        "        natural_language_response, json_output = response.split(\"JSON Output:\")\n",
        "        json_data = json.loads(json_output.strip())\n",
        "        return natural_language_response.strip(), json_data\n",
        "    except ValueError as e:\n",
        "        print(\"Error parsing response:\", e)\n",
        "        return response.strip(), None\n",
        "\n",
        "# Main interaction loop\n",
        "print(\"Welcome to the virtual coffee kiosk! What would you like to order?\")\n",
        "input_counter = 1\n",
        "\n",
        "while not order_confirmed:\n",
        "    # Take user input\n",
        "    user_input = input(\"Customer: \")\n",
        "\n",
        "    # Check if customer confirms the order\n",
        "    if \"주문 완료할게\" in user_input:\n",
        "        order_confirmed = True\n",
        "        print(\"Kiosk: 주문이 완료되었습니다. 결제는 카드리더기를 사용해주세요. 감사합니다.\")\n",
        "        conversation_history.clear()\n",
        "        summary_history.clear()\n",
        "        continue\n",
        "\n",
        "    # Add the latest user input to the conversation history\n",
        "    conversation_history.append(f\"Customer's {input_counter} Input: {user_input}\")\n",
        "    input_counter += 1\n",
        "\n",
        "    # Generate the prompt based on the conversation history and new user input\n",
        "    prompt = generate_prompt(conversation_history)\n",
        "\n",
        "    # Get the model's response\n",
        "    response = get_response(prompt)\n",
        "\n",
        "    # Parse the model's response\n",
        "    natural_language_response, json_output = parse_response(response)\n",
        "\n",
        "    # Save natural language order summary to `summary_history`\n",
        "    if natural_language_response:\n",
        "        summary_line = natural_language_response.split(\"\\n\", 1)[1] if \"\\n\" in natural_language_response else natural_language_response\n",
        "        summary_history.append(summary_line)\n",
        "\n",
        "    # Print the final natural language response for the customer\n",
        "    print(\"Kiosk:\", natural_language_response)\n",
        "\n",
        "    # Print the JSON output if it exists\n",
        "    if json_output:\n",
        "        print(\"JSON Output:\")\n",
        "        print(json.dumps(json_output, indent=4, ensure_ascii=False))\n",
        "\n",
        "    # Special handling for order summary requests\n",
        "    if \"내가 지금까지 뭘 주문했지\" in user_input:\n",
        "        # Provide cumulative order summary as a natural response without JSON output\n",
        "        print(\"Kiosk:\", \"지금까지의 주문내역입니다:\\n\" + \"\\n\".join(summary_history))\n",
        "\n",
        "    # Handle order cancellation requests\n",
        "    if \"주문 취소\" in user_input:\n",
        "        # Clear conversation history and summary history for a reset\n",
        "        print(\"Kiosk: 주문이 취소되었습니다. 새 주문을 시작해 주세요.\")\n",
        "        conversation_history.clear()\n",
        "        summary_history.clear()\n",
        "        input_counter = 1\n",
        "\n",
        "# End of the session\n",
        "print(\"Thank you for using the coffee kiosk!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Step 1: Load the LLaMA model and tokenizer\n",
        "model_path = \"meta-llama/Llama-3.2-1B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Create a text generation pipeline\n",
        "llama_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
        "# Step 2: Load the JSON file with chunks\n",
        "with open(\"kiosk_chunks.json\", \"r\", encoding=\"utf-8\") as file:\n",
        "    chunks_data = json.load(file)\n",
        "\n",
        "# Base Prompt - Minimized and concise\n",
        "BASE_PROMPT = \"\"\"\n",
        "You are a virtual assistant at a coffee kiosk. Respond to customer requests in natural Korean language and output the requested information in JSON format.\n",
        "\n",
        "### Task\n",
        "- Interpret customer requests, provide a concise Korean response, and generate a structured JSON output.\n",
        "- Reference provided examples for handling similar requests.\n",
        "\n",
        "JSON format for each response:\n",
        "{\n",
        "    \"action\": \"[action_type]\",\n",
        "    \"order_items\": [\n",
        "        {\n",
        "            \"drink\": \"[Drink Name]\",\n",
        "            \"size\": \"[Size]\",\n",
        "            \"temperature\": \"[Temperature]\",\n",
        "            \"quantity\": [Quantity],\n",
        "            \"add_ons\": [List of add-ons if any],\n",
        "            \"extra_shots\": [Number of extra shots if any]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "Proceed with the following examples and customer input.\n",
        "\"\"\"\n",
        "\n",
        "# Step 3: Define functions to find the best matching chunk and format the prompt\n",
        "\n",
        "def find_best_matching_chunk(user_input, chunks):\n",
        "    # Simple keyword-based matching\n",
        "    for chunk in chunks:\n",
        "        for example in chunk[\"examples\"]:\n",
        "            if any(keyword in user_input for keyword in example[\"input\"].split()):\n",
        "                return chunk\n",
        "    return None\n",
        "\n",
        "def format_prompt(matching_chunk, user_input):\n",
        "    instructions = matching_chunk[\"instructions\"]\n",
        "    examples = matching_chunk[\"examples\"][:2]  # Limit to 2 examples for brevity\n",
        "\n",
        "    # Format examples for the prompt\n",
        "    example_texts = \"\\n\".join([\n",
        "        f\"Example Input: '{ex['input']}'\\nResponse: '{ex['response']}'\\nJSON Output: {json.dumps(ex['json_output'], ensure_ascii=False)}\"\n",
        "        for ex in examples\n",
        "    ])\n",
        "\n",
        "    # Assemble the full prompt with base prompt, instructions, examples, and user input\n",
        "    prompt = f\"{BASE_PROMPT}\\n\\n### Chunk Instructions:\\n{instructions}\\n\\n### Examples:\\n{example_texts}\\n\\n### New Customer Input:\\n'{user_input}'\"\n",
        "    return prompt\n",
        "\n",
        "# Step 4: Generate a response from the LLaMA model\n",
        "def generate_response_from_model(prompt):\n",
        "    # Call the LLaMA model to generate a response based on the prompt\n",
        "    response = model(prompt, max_length=150, num_return_sequences=1, temperature=0.7)\n",
        "    generated_text = response[0][\"generated_text\"]\n",
        "    return generated_text[len(prompt):].strip()  # Strip prompt text from response\n",
        "\n",
        "# Step 5: Main function to process user input and produce the response\n",
        "def process_order(user_input):\n",
        "    # Find the best matching chunk for the user input\n",
        "    matching_chunk = find_best_matching_chunk(user_input, chunks_data)\n",
        "\n",
        "    if matching_chunk:\n",
        "        # Format the prompt based on the matching chunk\n",
        "        prompt = format_prompt(matching_chunk, user_input)\n",
        "\n",
        "        # Generate response from the model\n",
        "        response = generate_response_from_model(prompt)\n",
        "\n",
        "        # Print or return the response (including natural language and JSON)\n",
        "        print(\"Response:\", response)\n",
        "    else:\n",
        "        # Handle case where no specific chunk is found (e.g., a generic response or fallback)\n",
        "        print(\"Sorry, I didn’t understand that request. Can you please rephrase?\")\n",
        "\n",
        "# Example usage\n",
        "user_input = \"아이스 아메리카노 한 잔 줄 수 있나요?\"\n",
        "process_order(user_input)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "id": "5Ek4rJV_O8MI",
        "outputId": "6e7f83c6-6c2a-4d80-9eb5-a93cd3baff53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like meta-llama/Llama-3.2-1B is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1751\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1752\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1753\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1673\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1675\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    366\u001b[0m             )\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m:  (Request ID: Root=1-672ba71c-782356971b58a9a71e7f82e4;6f7950e4-0519-4bb0-8c23-b2004f4cd4be)\n\n403 Forbidden: Please enable access to public gated repositories in your fine-grained token settings to view this repository..\nCannot access content at: https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\nIf you are trying to create or update content, make sure you have a token with the `write` role.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1241\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \u001b[0;31m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1858\u001b[0;31m         raise LocalEntryNotFoundError(\n\u001b[0m\u001b[1;32m   1859\u001b[0m             \u001b[0;34m\"An error happened while trying to locate the file on the Hub and we cannot find the requested files\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-bdf7e3add552>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Step 1: Load the LLaMA model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"meta-llama/Llama-3.2-1B\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    875\u001b[0m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m                     config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    878\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    634\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m         ):\n\u001b[1;32m    445\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresolved_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    447\u001b[0m             \u001b[0;34mf\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this file, couldn't find it in the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0;34mf\" cached files and it looks like {path_or_repo_id} is not the path to a directory containing a file named\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like meta-llama/Llama-3.2-1B is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Enter your token here\n",
        "token = \"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\"\n",
        "login(token=token)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9lYoNx-g2T_",
        "outputId": "16880c13-4223-4cff-d7ce-dc28a759fb74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8yMg2ur3Z-8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/first fine_tuned Llama-3.2-3B-Instruct\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E1OMCfTZMRD",
        "outputId": "e907748b-94e7-4d9c-fec3-6c78cd08c1da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/first': No such file or directory\n",
            "rm: cannot remove 'fine_tuned': No such file or directory\n",
            "rm: cannot remove 'Llama-3.2-3B-Instruct': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "# Step 1: Load the LLaMA model and tokenizer, and move the model to the GPU\n",
        "model_path = \"meta-llama/Llama-3.2-1B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\")  # Move model to GPU\n",
        "\n",
        "# Step 2: Define a function to load chunks data\n",
        "with open(\"kiosk_chunks.json\", \"r\", encoding=\"utf-8\") as file:\n",
        "    chunks_data = json.load(file)\n",
        "\n",
        "# Define the Base Prompt - Minimally simplified for clarity\n",
        "BASE_PROMPT = \"\"\"\n",
        "You are a virtual assistant at a coffee kiosk. Interpret customer requests, provide a response in Korean, and output in JSON format.\n",
        "\n",
        "JSON format:\n",
        "{\n",
        "    \"action\": \"[action_type]\",\n",
        "    \"order_items\": [\n",
        "        {\n",
        "            \"drink\": \"[Drink Name]\",\n",
        "            \"size\": \"[Size]\",\n",
        "            \"temperature\": \"[Temperature]\",\n",
        "            \"quantity\": [Quantity],\n",
        "            \"add_ons\": [List of add-ons if any],\n",
        "            \"extra_shots\": [Number of extra shots if any]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "Proceed with the following examples and customer input.\n",
        "\"\"\"\n",
        "\n",
        "# Load a pretrained embeddings model (use \"all-MiniLM-L6-v2\" for efficiency)\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Precompute embeddings for each chunk example\n",
        "for chunk in chunks_data:\n",
        "    for example in chunk[\"examples\"]:\n",
        "        example[\"embedding\"] = embedding_model.encode(example[\"input\"], convert_to_tensor=True)\n",
        "\n",
        "def find_best_matching_chunk(user_input, chunks):\n",
        "    # Embed the user input\n",
        "    user_embedding = embedding_model.encode(user_input, convert_to_tensor=True)\n",
        "\n",
        "    # Track the best match\n",
        "    best_match = None\n",
        "    best_score = -1\n",
        "\n",
        "    # Find the most similar example based on cosine similarity\n",
        "    for chunk in chunks:\n",
        "        for example in chunk[\"examples\"]:\n",
        "            similarity = util.pytorch_cos_sim(user_embedding, example[\"embedding\"]).item()\n",
        "            if similarity > best_score:\n",
        "                best_score = similarity\n",
        "                best_match = chunk\n",
        "\n",
        "    return best_match\n",
        "\n",
        "\n",
        "def format_prompt(matching_chunk, user_input):\n",
        "    instructions = matching_chunk[\"instructions\"]\n",
        "    examples = matching_chunk[\"examples\"][:2]  # Limit to 2 examples for brevity\n",
        "    example_texts = \"\\n\".join([\n",
        "        f\"Example Input: '{ex['input']}'\\nResponse: '{ex['response']}'\\nJSON Output: {json.dumps(ex['json_output'], ensure_ascii=False)}\"\n",
        "        for ex in examples\n",
        "    ])\n",
        "    prompt = f\"{BASE_PROMPT}\\n\\n### Instructions:\\n{instructions}\\n\\n### Examples:\\n{example_texts}\\n\\n### New Customer Input:\\n'{user_input}'\"\n",
        "    return prompt\n",
        "\n",
        "# Step 4: Generate response from the model using tokenized input\n",
        "def generate_response_from_model(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Move tensors to GPU\n",
        "    outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.7)  # Set max_new_tokens instead of max_length\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text[len(prompt):].strip()  # Strip prompt text from response\n",
        "\n",
        "# Step 5: Main function to process user input and produce the response\n",
        "def process_order(user_input):\n",
        "    matching_chunk = find_best_matching_chunk(user_input, chunks_data)\n",
        "    if matching_chunk:\n",
        "        prompt = format_prompt(matching_chunk, user_input)\n",
        "        response = generate_response_from_model(prompt)\n",
        "        print(\"Response:\", response)\n",
        "    else:\n",
        "        print(\"Sorry, I didn’t understand that request. Can you please rephrase?\")\n",
        "\n",
        "# Example usage\n",
        "user_input = \"아이스 아메리카노 휘핑 크림 추가해서 줘요\"\n",
        "process_order(user_input)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215,
          "referenced_widgets": [
            "243f63189a804e6eaa62fb2937df35db",
            "82e76ff9d722477fa1e20429b1d09995",
            "7786591554a649bf9e6b65fe53e8836e",
            "723f2be3b7dd46f48b0d924f80fcdff6",
            "2f134f94babe4f4fa3980efa5efe9bc2",
            "50667f8107c54974b4c79742f7ee33c3",
            "3d404e4fff1547278e778e95daf02adb",
            "17e5494c9d8e480a8dca11057620ed85",
            "5190da853f794e7a9846643fb172c905",
            "3c67b936763545a692056c99aa28da67",
            "535df98171484be5a98c4f639948579c"
          ]
        },
        "id": "p1M1J35bSXQR",
        "outputId": "b4abe28f-cb84-440a-d593-e71b7302a8de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "243f63189a804e6eaa62fb2937df35db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Recipe: 아이스 아메리카노 - 휘핑 크림 4잔 추가\n",
            "<|endoftext|>JSON Output: {\"action\": \"add_item\", \"order_id\": 1, \"drink\": \"아메리카\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Step 1: Define the chunks containing relevant information\n",
        "chunks = {\n",
        "    \"menu_items\": \"\"\"\n",
        "    Menu Items:\n",
        "    - Hot Drinks (always hot): 허브티\n",
        "    - Iced Only Drinks (always iced): 토마토주스, 키위주스, 망고스무디, 딸기스무디, 레몬에이드, 복숭아아이스티\n",
        "    - Hot/Iced Coffee (available in both hot and iced): 아메리카노, 라떼, 카푸치노, 카페모카, 바닐라라떼, 에스프레소, 카라멜마끼아또\n",
        "    - Specialty Drinks (available in hot or iced): 초콜릿라떼\n",
        "    - Available sizes: 미디움, 라지, 엑스라지\n",
        "    \"\"\",\n",
        "    \"default_values\": \"\"\"\n",
        "    Default Order Values:\n",
        "    - If no size is specified, use \\\"미디움\\\" by default.\n",
        "    - If no temperature is specified, use \\\"핫\\\" by default unless the drink is iced-only.\n",
        "    \"\"\",\n",
        "    \"add_ons\": \"\"\"\n",
        "    Add-Ons Options:\n",
        "    - Available add-ons: 휘핑크림 (Whipped Cream), 바닐라시럽 (Vanilla Syrup), 카라멜시럽 (Caramel Syrup), 샷 (Extra Shot)\n",
        "    \"\"\",\n",
        "    \"actions_json_format\": \"\"\"\n",
        "    Available Actions and JSON Formats:\n",
        "\n",
        "    1. **Create Order**:\n",
        "       JSON Format:\n",
        "       {\n",
        "         \\\"action\\\": \\\"create_order\\\",\n",
        "         \\\"order_items\\\": [\n",
        "           {\n",
        "             \\\"drink\\\": \\\"[Drink Name]\\\",\n",
        "             \\\"size\\\": \\\"[Size]\\\",\n",
        "             \\\"temperature\\\": \\\"[Temperature]\\\",\n",
        "             \\\"quantity\\\": [Quantity],\n",
        "             \\\"add_ons\\\": [List of add-ons if any],\n",
        "             \\\"extra_shots\\\": [Number of extra shots if any]\n",
        "           }\n",
        "         ]\n",
        "       }\n",
        "\n",
        "    2. **Add Item**:\n",
        "       JSON Format:\n",
        "       {\n",
        "         \\\"action\\\": \\\"add_item\\\",\n",
        "         \\\"order_items\\\": [\n",
        "           {\n",
        "             \\\"drink\\\": \\\"[Drink Name]\\\",\n",
        "             \\\"size\\\": \\\"[Size]\\\",\n",
        "             \\\"temperature\\\": \\\"[Temperature]\\\",\n",
        "             \\\"quantity\\\": [Quantity],\n",
        "             \\\"add_ons\\\": [List of add-ons if any],\n",
        "             \\\"extra_shots\\\": [Number of extra shots if any]\n",
        "           }\n",
        "         ]\n",
        "       }\n",
        "\n",
        "    3. **Modify Order**:\n",
        "       JSON Format:\n",
        "       {\n",
        "         \\\"action\\\": \\\"modify_order\\\",\n",
        "         \\\"old_drink\\\": \\\"[Previous Drink Name]\\\",\n",
        "         \\\"new_drink\\\": \\\"[Updated Drink Name]\\\",\n",
        "         \\\"size\\\": \\\"[Size]\\\",\n",
        "         \\\"temperature\\\": \\\"[Temperature]\\\",\n",
        "         \\\"quantity\\\": [Quantity],\n",
        "         \\\"add_ons\\\": [List of add-ons if any],\n",
        "         \\\"extra_shots\\\": [Number of extra shots if any]\n",
        "       }\n",
        "\n",
        "    4. **Cancel Order**:\n",
        "       JSON Format:\n",
        "       {\n",
        "         \\\"action\\\": \\\"cancel_order\\\",\n",
        "         \\\"order_items\\\": []\n",
        "       }\n",
        "\n",
        "    5. **Show Order Summary**:\n",
        "       JSON Format: None (This action returns a natural language summary only).\n",
        "\n",
        "    6. **Recommend Closest Item**:\n",
        "       JSON Format:\n",
        "       {\n",
        "         \\\"action\\\": \\\"recommend_closest_item\\\",\n",
        "         \\\"requested_item\\\": \\\"[Requested Item]\\\",\n",
        "         \\\"recommended_item\\\": \\\"[Recommended Item]\\\"\n",
        "       }\n",
        "\n",
        "    7. **Complete Order**:\n",
        "       JSON Format:\n",
        "       {\n",
        "         \\\"action\\\": \\\"complete_order\\\",\n",
        "         \\\"order_items\\\": [Summary of all items ordered]\n",
        "       }\n",
        "\n",
        "    8. **Show Menu**:\n",
        "       JSON Format: None (Displays the available items in natural language).\n",
        "    \"\"\",\n",
        "    \"unavailable_items_handling\": \"\"\"\n",
        "    Unavailable Items Handling:\n",
        "    - If the customer requests an item not on the menu, respond with:\n",
        "      \\\"죄송합니다, 해당 메뉴는 없습니다.\\\" (\\\"Sorry, that item is not on the menu.\\\")\n",
        "    \"\"\",\n",
        "    \"common_misspellings\": \"\"\"\n",
        "    Common Misspellings and Short Names:\n",
        "    - \\\"아아\\\": 아이스 아메리카노 (Iced Americano)\n",
        "    - \\\"뜨아\\\": 핫 아메리카노 (Hot Americano)\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "# Step 2: Define a function to retrieve relevant chunks based on customer input\n",
        "def retrieve_relevant_chunks(customer_input):\n",
        "    relevant_chunks = []\n",
        "    if any(drink in customer_input for drink in [\"허브티\", \"아메리카노\", \"라떼\", \"카푸치노\", \"카페모카\", \"바닐라라떼\", \"에스프레소\", \"카라멜마끼아또\", \"초콜릿라떼\", \"토마토주스\", \"키위주스\", \"망고스무디\", \"딸기스무디\", \"레몬에이드\", \"복숭아아이스티\"]):\n",
        "        relevant_chunks.append(chunks[\"menu_items\"])\n",
        "    if \"휘핑크림\" in customer_input or \"바닐라시럽\" in customer_input or \"카라멜시럽\" in customer_input or \"샷\" in customer_input:\n",
        "        relevant_chunks.append(chunks[\"add_ons\"])\n",
        "    if \"주문\" in customer_input:\n",
        "        relevant_chunks.append(chunks[\"actions_json_format\"])\n",
        "    if \"없습니다\" in customer_input:\n",
        "        relevant_chunks.append(chunks[\"unavailable_items_handling\"])\n",
        "    if any(short_name in customer_input for short_name in [\"아아\", \"뜨아\"]):\n",
        "        relevant_chunks.append(chunks[\"common_misspellings\"])\n",
        "    return relevant_chunks\n",
        "\n",
        "# Step 3: Define a function to generate the prompt for the model\n",
        "def generate_prompt(conversation_history, customer_input):\n",
        "    relevant_chunks = retrieve_relevant_chunks(customer_input)\n",
        "    formatted_chunks = \"\\n\".join(relevant_chunks)\n",
        "    base_prompt = f\"\"\"\n",
        "    You are operating a virtual coffee kiosk that receives STT (speech-to-text) inputs from customers placing coffee orders.\n",
        "\n",
        "    Current Conversation History: {conversation_history}\n",
        "\n",
        "    {formatted_chunks}\n",
        "\n",
        "    Response:\n",
        "    \"\"\"\n",
        "    return base_prompt.strip()\n",
        "\n",
        "# Example usage\n",
        "conversation_history = \"Customer's 1 Input: 아이스 한잔하고 핫 아메리카노 두잔 주세요\"\n",
        "customer_input = \"아이스 카페모카 한잔 주세요\"\n",
        "prompt = generate_prompt(conversation_history, customer_input)\n",
        "print(prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWA8EfII4G8M",
        "outputId": "3fa156f9-d824-4251-fe3b-b651ca9f68d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are operating a virtual coffee kiosk that receives STT (speech-to-text) inputs from customers placing coffee orders.\n",
            "\n",
            "    Current Conversation History: Customer's 1 Input: 아이스 한잔하고 핫 아메리카노 두잔 주세요\n",
            "\n",
            "    \n",
            "    Menu Items:\n",
            "    - Hot Drinks (always hot): 허브티\n",
            "    - Iced Only Drinks (always iced): 토마토주스, 키위주스, 망고스무디, 딸기스무디, 레몬에이드, 복숭아아이스티\n",
            "    - Hot/Iced Coffee (available in both hot and iced): 아메리카노, 라떼, 카푸치노, 카페모카, 바닐라라떼, 에스프레소, 카라멜마끼아또\n",
            "    - Specialty Drinks (available in hot or iced): 초콜릿라떼\n",
            "    - Available sizes: 미디움, 라지, 엑스라지\n",
            "    \n",
            "\n",
            "    Response:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "# Define chunks with detailed information\n",
        "chunks = {\n",
        "    \"menu_items\": \"The kiosk offers the following drinks: Hot Drinks: 허브티 (always hot), Iced Only Drinks: 토마토주스, 키위주스, 망고스무디, etc. Available sizes: 미디움, 라지, 엑스라지. Available add-ons: 휘핑크림, 바닐라시럽, 카라멜시럽, 샷.\",\n",
        "    \"actions_json_format\": \"Available actions for JSON output: create_order, add_item, modify_order, cancel_order, recommend_closest_item, show_order_summary, complete_order. JSON output format: {\\\"action\\\": \\\"[action_type]\\\", \\\"order_items\\\": [...]}\",\n",
        "    \"default_values\": \"Default size is \\\"미디움\\\" and default temperature is \\\"핫\\\" if unspecified. Do not override explicitly given details.\",\n",
        "    \"common_misspellings\": \"Common shorthand: \\\"아아\\\" means \\\"아이스 아메리카노\\\", \\\"뜨아\\\" means \\\"핫 아메리카노\\\".\",\n",
        "    \"order_flow_examples\": \"Example flows: Create order, modify existing order, add items, request summary, complete order. Example: '아메리카노 한잔 주세요' means create an order for one Americano.\"\n",
        "}\n",
        "\n",
        "# Load a pre-trained embedding model\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Create embeddings for each chunk\n",
        "chunk_embeddings = {key: embedding_model.encode(value, convert_to_tensor=True) for key, value in chunks.items()}\n",
        "\n",
        "#### Step 2: Function to Retrieve Relevant Chunks Based on Customer Input\n",
        "\n",
        "def retrieve_relevant_chunks(customer_input, chunk_embeddings, top_k=3):\n",
        "    # Embed the customer input\n",
        "    input_embedding = embedding_model.encode(customer_input, convert_to_tensor=True)\n",
        "\n",
        "    # Calculate cosine similarity between the input and all chunks\n",
        "    similarities = {}\n",
        "    for key, chunk_embedding in chunk_embeddings.items():\n",
        "        similarity = util.pytorch_cos_sim(input_embedding, chunk_embedding)\n",
        "        similarities[key] = similarity.item()\n",
        "\n",
        "    # Sort chunks by similarity and select the top_k most relevant ones\n",
        "    sorted_chunks = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "    relevant_chunks = [chunks[key] for key, _ in sorted_chunks[:top_k]]\n",
        "\n",
        "    return relevant_chunks\n",
        "\n",
        "#### Step 3: Generate a Prompt with the Retrieved Chunks\n",
        "\n",
        "def generate_prompt(customer_input):\n",
        "    relevant_chunks = retrieve_relevant_chunks(customer_input, chunk_embeddings)\n",
        "\n",
        "    # Combine the retrieved chunks into a prompt\n",
        "    prompt = \"\\n\".join(relevant_chunks) + f\"\\nCustomer Input: {customer_input}\\nResponse:\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Example Customer Input\n",
        "customer_input = \"아이스 한잔하고 핫 아메리카노 두잔 바꾸어주세요\"\n",
        "\n",
        "# Generate a prompt with the most relevant chunks\n",
        "prompt = generate_prompt(customer_input)\n",
        "print(prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84zF3VOn69ve",
        "outputId": "aa7c6526-241e-47b7-bef4-601da3f279bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common shorthand: \"아아\" means \"아이스 아메리카노\", \"뜨아\" means \"핫 아메리카노\".\n",
            "Example flows: Create order, modify existing order, add items, request summary, complete order. Example: '아메리카노 한잔 주세요' means create an order for one Americano.\n",
            "The kiosk offers the following drinks: Hot Drinks: 허브티 (always hot), Iced Only Drinks: 토마토주스, 키위주스, 망고스무디, etc. Available sizes: 미디움, 라지, 엑스라지. Available add-ons: 휘핑크림, 바닐라시럽, 카라멜시럽, 샷.\n",
            "Customer Input: 아이스 한잔하고 핫 아메리카노 두잔 바꾸어주세요\n",
            "Response:\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f838f19d08f040deaa9c0207612218af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be4852d0f365488e96cbd0dbaa58325a",
              "IPY_MODEL_8652c27ad5b0453f92c6065a1b1dc275",
              "IPY_MODEL_0af4cd9f0fb84801b9be6b6ccf6bba0c"
            ],
            "layout": "IPY_MODEL_e3afd7bbb16e4d8aa2fe9640af747cbb"
          }
        },
        "be4852d0f365488e96cbd0dbaa58325a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0efcd5cfcc44915a15ade2c2ac00a62",
            "placeholder": "​",
            "style": "IPY_MODEL_8da10b38669341ed94b930c2aacb1cb0",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8652c27ad5b0453f92c6065a1b1dc275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d47ef1e7e8204cfa900fa7c7dda6b8e1",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c63d6757b3274801beafffa900af4b0d",
            "value": 2
          }
        },
        "0af4cd9f0fb84801b9be6b6ccf6bba0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b443a35906ee4836be58190eb9717b4e",
            "placeholder": "​",
            "style": "IPY_MODEL_5b5f1050f5274e2c914a9920de9ee198",
            "value": " 2/2 [00:04&lt;00:00,  2.08s/it]"
          }
        },
        "e3afd7bbb16e4d8aa2fe9640af747cbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0efcd5cfcc44915a15ade2c2ac00a62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8da10b38669341ed94b930c2aacb1cb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d47ef1e7e8204cfa900fa7c7dda6b8e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c63d6757b3274801beafffa900af4b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b443a35906ee4836be58190eb9717b4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b5f1050f5274e2c914a9920de9ee198": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "243f63189a804e6eaa62fb2937df35db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82e76ff9d722477fa1e20429b1d09995",
              "IPY_MODEL_7786591554a649bf9e6b65fe53e8836e",
              "IPY_MODEL_723f2be3b7dd46f48b0d924f80fcdff6"
            ],
            "layout": "IPY_MODEL_2f134f94babe4f4fa3980efa5efe9bc2"
          }
        },
        "82e76ff9d722477fa1e20429b1d09995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50667f8107c54974b4c79742f7ee33c3",
            "placeholder": "​",
            "style": "IPY_MODEL_3d404e4fff1547278e778e95daf02adb",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7786591554a649bf9e6b65fe53e8836e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17e5494c9d8e480a8dca11057620ed85",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5190da853f794e7a9846643fb172c905",
            "value": 2
          }
        },
        "723f2be3b7dd46f48b0d924f80fcdff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c67b936763545a692056c99aa28da67",
            "placeholder": "​",
            "style": "IPY_MODEL_535df98171484be5a98c4f639948579c",
            "value": " 2/2 [00:04&lt;00:00,  2.10s/it]"
          }
        },
        "2f134f94babe4f4fa3980efa5efe9bc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50667f8107c54974b4c79742f7ee33c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d404e4fff1547278e778e95daf02adb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17e5494c9d8e480a8dca11057620ed85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5190da853f794e7a9846643fb172c905": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c67b936763545a692056c99aa28da67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "535df98171484be5a98c4f639948579c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}